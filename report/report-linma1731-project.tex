%%% ====================================================================
%%% @LaTeX-file{
%%%   filename  = "aomsample.tex",
%%%   copyright = "Copyright 1995, 1999 American Mathematical Society,
%%%                2005 Hebrew University Magnes Press,
%%%                all rights reserved.  Copying of this file is
%%%                authorized only if either:
%%%                (1) you make absolutely no changes to your copy,
%%%                including name; OR
%%%                (2) if you do make changes, you first rename it
%%%                to some other name.",
%%% }
%%% ====================================================================
\NeedsTeXFormat{LaTeX2e}% LaTeX 2.09 can't be used (nor non-LaTeX)
[1994/12/01]% LaTeX date must December 1994 or later
\documentclass[final]{aomart}
\usepackage[english]{babel}

\usepackage{mathtools,amssymb,amsthm}
\usepackage{bm}

%    Some definitions useful in producing this sort of documentation:
\chardef\bslash=`\\ % p. 424, TeXbook
%    Normalized (nonbold, nonitalic) tt font, to avoid font
%    substitution warning messages if tt is used inside section
%    headings and other places where odd font combinations might
%    result.
\newcommand{\ntt}{\normalfont\ttfamily}
%    command name
\newcommand{\cn}[1]{{\protect\ntt\bslash#1}}
%    LaTeX package name
\newcommand{\pkg}[1]{{\protect\ntt#1}}
%    File name
\newcommand{\fn}[1]{{\protect\ntt#1}}
%    environment name
\newcommand{\env}[1]{{\protect\ntt#1}}
\hfuzz1pc % Don't bother to report overfull boxes if overage is < 1pc

%       Theorem environments

%% \theoremstyle{plain} %% This is the default
\newtheorem[{}\it]{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Property}
\newtheorem{propo}[thm]{Proposition}
\newtheorem{ax}{Axiom}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem*[{}\it]{notation}{Notation}
\newtheorem{step}{Step}

\numberwithin{equation}{section}

\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\secref}[1]{\S\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}


%       Math definitions

\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\st}{\sigma}
\newcommand{\XcY}{{(X,Y)}}
\newcommand{\SX}{{S_X}}
\newcommand{\SY}{{S_Y}}
\newcommand{\SXY}{{S_{X,Y}}}
\newcommand{\SXgYy}{{S_{X|Y}(y)}}
\newcommand{\Cw}[1]{{\hat C_#1(X|Y)}}
\newcommand{\G}{{G(X|Y)}}
\newcommand{\PY}{{P_{\mathcal{Y}}}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\wt}{\widetilde}
\newcommand{\wh}{\widehat}


\newcommand{\like}{\mathcal{L}} % likelihood
\newcommand{\loglike}{\ell} % log-likelihood
\newcommand{\e}{\mathrm{e}} % euler's constant
\newcommand{\pdf}{f} % probability density function
\newcommand{\htheta}{\wh{\theta}} % estimator (abuse of notation)
\newcommand{\hTheta}{\wh{\Theta}} % estimator
\DeclareMathOperator{\newdiff}{d} % use \dif instead
\newcommand{\dif}{\newdiff\!} % differential operator
\newcommand{\fisher}{\mathcal{I}} % fisher information matrix
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\dom}{dom}

\makeatletter
\DeclareRobustCommand{\expe}{\mathbf{E}\@ifstar\@firstofone\@expe}
\newcommand{\@expe}[1]{\left[#1\right]}
\makeatother

%    \interval is used to provide better spacing after a [ that
%    is used as a closing delimiter.
\newcommand{\interval}[1]{\mathinner{#1}}

%    Notation for an expression evaluated at a particular condition. The
%    optional argument can be used to override automatic sizing of the
%    right vert bar, e.g. \eval[\biggr]{...}_{...}
\newcommand{\eval}[2][\right]{\relax
  \ifx#1\right\relax \left.\fi#2#1\rvert}

%    Enclose the argument in vert-bar delimiters:
\newcommand{\envert}[1]{\left\lvert#1\right\rvert}
\let\abs=\envert

%    Enclose the argument in double-vert-bar delimiters:
\newcommand{\enVert}[1]{\left\lVert#1\right\rVert}
\let\norm=\enVert

%\setcounter{tocdepth}{5}

\title[Fish schools tracking]{LINMA1731 -- Project 2019\\
Fish schools tracking}

\author{Gilles Peiffer}
\address{Université catholique de Louvain, Ottignies-Louvain-la-Neuve, Belgium}
\fulladdress{École Polytechnique\\
	Université catholique de Louvain\\
	Place de l'Université 1, 1348 Ottignies-Louvain-la-Neuve, Belgium}
\email{gilles.peiffer@student.uclouvain.be}
\givenname{Gilles}
\surname{Peiffer}

\author{Louis Navarre}
\address{Université catholique de Louvain, Ottignies-Louvain-la-Neuve, Belgium}
\fulladdress{École Polytechnique, Université catholique de Louvain, Place de l'Université 1, 1348 Ottignies-Louvain-la-Neuve, Belgique}
\email{navarre.louis@student.uclouvain.be}
\givenname{Louis}
\surname{Navarre}

%\oldsubsections
\copyrightnote{\textcopyright~2019 Gilles Peiffer and Louis Navarre}

\begin{document}

\begin{abstract}
	In this paper we solve the first part of the project for the class ``Stochastic process: Estimation and prediction'' given during the Fall term of 2019.
	The average speed of each fish in a school of fish is approximated by a gamma-distributed random variable with a shape parameter \(k\) and a scale parameter \(s\), and various methods for estimating this quantity are given; a numerical simulation is also included.
\end{abstract}

\maketitle
\tableofcontents

\part{Average speed estimation}
\section{Introduction}
For the purpose of this project, we assume that the speed of each fish in a school
at time \(i\) is a random variable \(V_i\) following a Gamma distribution, as suggested in [1].
This distribution is characterized by two parameters:
a shape parameter \(k > 0\) and a scale parameter \(s > 0\).
The parameters are the same for every fish and are time invariant.
The aim of this first part is to identify these two parameters using empirical observations \(v_i\).

\section{Maximum likelihood estimation}
Let \(v_i\) be i.i.d. realisations of a random variable following a Gamma distribution \(\Gamma(k, s)\) (with \(i = 1,\ldots, N)\).
We first assume that the shape parameter \(k\) is known.

We start by deriving the maximum likelihood estimator of \(\theta \coloneqq s\) based on \(N\) observations.
Since the estimand $\theta$ is a deterministic quantity, we use Fisher estimation. % TODO is this correct?
In order to do this, let us restate the probability density function of \(V_i \sim \Gamma(k, s)\):
\begin{equation}
\pdf_{V_i}(v_i; k, s) = \frac{1}{\Gamma(k) s^k} v_i^{k-1} \e^{-\frac{v_i}{s}}\,, \quad i = 1, \ldots, N\,.
\end{equation}
With this in mind, we can find that the likelihood \(\like(v_1, \ldots, v_N; k, \theta)\) is given by
\begin{align}
\like(v_1, \ldots, v_N; k, \theta) &= \prod_{i=1}^{N} \pdf_{V_i}(v_i; k,\theta)\\
&= \prod_{i=1}^{N} \frac{1}{\Gamma(k) \theta^k} v_i^{k-1} \e^{-\frac{v_i}{\theta}}\,.
\end{align}
In order to alleviate notation, we compute instead the log-likelihood, which is generally easier to work with\footnote{This is possible because the values of \(\theta\) which maximize the log-likelihood also maximize the likelihood.}:
\begin{align}
\ell(v_1, \ldots, v_N; k, \theta) &\coloneqq \ln \like(v_1, \ldots, v_N; k, \theta)\\
&= \ln\Bigg(\prod_{i=1}^{N} \frac{1}{\Gamma(k) \theta^k} v_i^{k-1} \e^{-\frac{v_i}{\theta}} \Bigg)\\
& = \sum_{i=1}^{N} \ln\Bigg(\frac{1}{\Gamma(k) \theta^k} v_i^{k-1} \e^{-\frac{v_i}{\theta}}\Bigg)\\
& = (k-1) \sum_{i=1}^{N}\ln v_i - \sum_{i=1}^{N} \frac{v_i}{\theta} - N \big(k \ln \theta + \ln \Gamma(k)\big)\,.
\end{align}
Now, in order to obtain the maximum likelihood estimate \(\htheta\), we must differentiate the log-likelihood with respect to the estimand \(\theta\), and set it equal to zero:
\begin{align}
\eval{\frac{\partial \ell(v_1, \ldots, v_N; k, \theta)}{\partial \theta}}_{\theta = \htheta} &= -\frac{kN}{\htheta} + \frac{\sum_{i=1}^{N} v_i}{\htheta^2} = 0\\
\iff \htheta &= \frac{\sum_{i=1}^{N} v_i}{kN} = \frac{\widebar{v}}{k}\,.\\
\intertext{This then allows us to find the maximum-likelihood estimator \(\hTheta\), given by}
\hTheta &= \frac{\sum_{i=1}^{N} V_i}{kN} = \frac{\widebar{V}}{k}\,.\label{eq:mlestimator}
\end{align}

\section{Properties of the estimator}
We now wish to show some of the properties of this estimator.
\subsection{Asymptotically unbiased}
\begin{defn}[Unbiased estimator]
The Fisher estimator \(\hTheta = g(Z)\) of \(\theta\) is \emph{unbiased} if
\begin{equation}
m_{\hTheta; \theta} \coloneqq \expe{g(Z); \theta} = \theta\,, \quad \textnormal{for all } \theta\,,
\end{equation}
where
\begin{equation}
\expe{g(Z); \theta} \coloneqq \int_{\dom Z} g(Z) \pdf_Z(z; \theta) \dif z\,.
\end{equation}
\end{defn}
\begin{prop}
The maximum likelihood estimator derived in \eqref{eq:mlestimator} is asymptotically unbiased, that is,
\begin{equation}
\lim_{N \to +\infty} \expe{g(V_1, \ldots, V_n); \theta} = \theta\,.
\end{equation}
\end{prop}
\begin{proof}
We wish to prove that
\begin{equation}
\lim_{N \to +\infty} \expe{\frac{\widebar{V}}{k}} = \theta\,.
\end{equation}
We recall that \(\expe{V_i} = k \theta\) for \(V_i \sim \Gamma(k, \theta)\)
and that the expected value operator is linear to obtain that
\begin{equation}
\expe{\frac{\widebar{V}}{k}} = \frac{\expe{\frac{1}{N} \sum_{i=1}^N V_i}}{k} = \frac{\frac{1}{N} \sum_{i=1}^N \expe{V_i}}{k} = \frac{\frac{1}{N} N k \theta}{k} = \theta\,.
\end{equation}
This proves that the maximum likelihood estimator of \eqref{eq:mlestimator} is unbiased,
hence it is also asymptotically unbiased.
\end{proof}
\subsection{Efficiency}
\begin{thm}[Cramér--Rao inequality]
If \(Z = (Z_1, \ldots, Z_N)^T\) with i.i.d. random variables \(Z_k\) and if its probability density function given by \(\pdf_Z(z; \theta) = \prod_{k=1}^{N} \pdf_{Z_k}(z_k; \theta)\) satisfies the following regularity condition:
\begin{equation}
\expe{\frac{\partial \pdf_Z(z; \theta)}{\partial \theta}} = \int_{-\infty}^{+\infty} \frac{\partial \pdf_Z(z; \theta)}{\partial \theta} \pdf_Z(z; \theta) \dif z \,, \quad \forall \theta\,,
\end{equation}
then the covariance of any unbiased estimator \(\hTheta\) satisfies the \emph{Cramér--Rao inequality}
\begin{equation}
\cov \hTheta \ge \fisher^{-1}(\theta)\,,
\end{equation}
where \(\fisher(\theta)\) is the \(N \times N\) \emph{Fisher information matrix},
defined by
\begin{equation}
\big[\fisher(\theta)\big]_{i, j} \coloneqq -\expe{\frac{\partial^2 \ln \pdf_Z(z; \theta)}{\partial \theta_i \partial \theta_j}}\,.
\label{information_matrix}
\end{equation}
\end{thm}
\begin{defn}[Efficient estimator]
An estimator is said to be \emph{efficient} if it reaches the Cramér--Rao bound for all values of \(\theta\), that is,
\begin{equation}
\cov \hTheta = \fisher^{-1}(\theta)\,, \quad \forall \theta\,.
\end{equation}
\end{defn}
\begin{prop}
The maximum likelihood estimator derived in \eqref{eq:mlestimator} is efficient.
\end{prop}
\begin{proof}

\end{proof}
\subsection{Best asymptotically normal}
\begin{prop}
	The maximum likelihood estimator is best asymptotically normal.
\end{prop}
\begin{proof}
	The proof is trivial and left as an exercise to the reader.
\end{proof}
\subsection{Consistent}
\begin{prop}
	The maximum likelihood estimator is consistent.
\end{prop}
\begin{proof}
	The proof is trivial and left as an exercise to the reader.
\end{proof}
\section{Joint maximum likelihood estimation}
We now consider \(V_i \sim \Gamma(k, s)\) (for \(i = 1,\ldots,N)\) with both \(k\) and \(s\) unknown.
\section{Numerical simulation}
\section{Fisher information matrix}
We can compute the Fisher information matrix. The entry $(i,j)$ of this matrix is given by equation \ref{information_matrix}. Since we have two estimators, this matrix is a $2\times 2$ matrix. Remember that
\begin{equation}
	\begin{aligned}
	lnf_Z(z,\theta) = h(z,\theta) & = ln\bigg( \frac{1}{\Gamma(k)s^k}x^{k-1}e^{-x/s} \bigg)\\
						   & = ln(x^{k-1}) + ln(e^{-x/s}) - ln(\Gamma(k)) - ln(s^k)\\
						   & = (k-1)ln(x) - \frac{x}{s} - ln(\Gamma(k)) - kln(s).
	\end{aligned}
\end{equation}
Therefore, before calculating the entries of the information matrix, we have to compute the partial derivatives before taking the expectation of it. These are given by equations \ref{00}, \ref{01}, \ref{10} and \ref{11}
\begin{equation}
	\begin{aligned}
	\frac{\partial h(z;\theta)}{\partial s} & = \frac{x}{s^2} - \frac{k}{s}
	\end{aligned}
\end{equation}
\begin{equation}
	\begin{aligned}
	\frac{\partial h(z;\theta)}{\partial k} & = ln(x) - \frac{1}{\Gamma(k)}\Gamma(k)\phi^{(0)}(k) - ln(s)\\
														  & = ln(x) - \phi^{0}(k) - ln(s).
	\end{aligned}
\end{equation}
\begin{equation}
	\begin{aligned}
	\frac{\partial^2h(z;\theta)}{\partial s^2} & = -\frac{x}{s^3}\cdot\frac{1}{2} + \frac{k}{s^2}.
	\end{aligned}
	\label{00}
\end{equation}
\begin{equation}
	\begin{aligned}
	\frac{\partial^2 h(z;\theta)}{\partial k \partial s} & = -\frac{1}{s}.
	\end{aligned}
	\label{01}
\end{equation}
\begin{equation}
	\begin{aligned}
	\frac{\partial h(z; \theta)}{\partial s \partial k} & = -\frac{1}{s}.
	\end{aligned}
	\label{10}
\end{equation}
\begin{equation}
	\begin{aligned}
	\frac{\partial h(z; \theta)}{\partial k^2} & = -\frac{d\phi^{(0)}(k)}{dk}\\
															   & = -\frac{d^2\Gamma(k)}{dk^2}\\
															   & = -\phi^{(1)}(k).
	\end{aligned}
	\label{11}
\end{equation}
Then, we take the expectation of these computed values.
\begin{equation}
	\begin{aligned}
	\fisher_{00} & = -\mathbb{E}\bigg\{ \frac{\partial^2 h(z;\theta)}{\partial s^2} \bigg\}\\
					   & = -\mathbb{E}\bigg\{ \frac{x}{s^2} - \frac{k}{s} \bigg\}\\
					   & = \frac{k}{s} - \frac{\mathbb{E}\{ x \}}{s^2}\\
					   & = \frac{k}{s} - \frac{ks}{s^2}\\
					   & = 0.
	\end{aligned}
\end{equation}
\begin{equation}
	\begin{aligned}
	\fisher_{01} & = -\mathbb{E}\bigg\{ \frac{\partial^2 h(z;\theta)}{\partial k\partial s} \bigg\}\\
					  & = -\mathbb{E}\bigg\{ -\frac{1}{s} \bigg\}\\
					  & = \frac{1}{s}.
	\end{aligned}
\end{equation}
\begin{equation}
	\begin{aligned}
	\fisher_{10} & = \fisher{01} = \frac{1}{s}.
	\end{aligned}
\end{equation}
\begin{equation}
	\begin{aligned}
	\fisher_{11} & = -\mathbb{E}\bigg\{ \frac{\partial^2 h(z;\theta)}{\partial k^2} \bigg\}\\
					  & = -\mathbb{E}bigg\{ \phi^{(1)}(k) \bigg\}\\
					  & = -\phi^{(1)}(k).
	\end{aligned}
\end{equation}
Finally, the Fisher information matrix is 
\[ \fisher(\theta) = \left( \begin{array}{cc}
	0 & \frac{1}{s} \\
	\frac{1}{s} & -\phi^{(1)}(k) \\
	 \end{array} \right)\] 
\section{Numerical proof}
\bibliography{report-linma1731-project}
\bibliographystyle{aomalpha}

\end{document}
\endinput
