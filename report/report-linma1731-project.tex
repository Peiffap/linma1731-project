%%% ====================================================================
%%% @LaTeX-file{
%%%   filename  = "aomsample.tex",
%%%   copyright = "Copyright 1995, 1999 American Mathematical Society,
%%%                2005 Hebrew University Magnes Press,
%%%                all rights reserved.  Copying of this file is
%%%                authorized only if either:
%%%                (1) you make absolutely no changes to your copy,
%%%                including name; OR
%%%                (2) if you do make changes, you first rename it
%%%                to some other name.",
%%% }
%%% ====================================================================
\NeedsTeXFormat{LaTeX2e}% LaTeX 2.09 can't be used (nor non-LaTeX)
[1994/12/01]% LaTeX date must December 1994 or later
\documentclass[final]{aomart}
\usepackage[english]{babel}

\usepackage{mathtools,amssymb,amsthm, mathrsfs}
\usepackage{bm}
\usepackage{float}
\usepackage{siunitx}
\usepackage{minted}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepackage{subcaption}
\usepackage{url}

%    Some definitions useful in producing this sort of documentation:
\chardef\bslash=`\\ % p. 424, TeXbook
%    Normalized (nonbold, nonitalic) tt font, to avoid font
%    substitution warning messages if tt is used inside section
%    headings and other places where odd font combinations might
%    result.
\newcommand{\ntt}{\normalfont\ttfamily}
%    command name
\newcommand{\cn}[1]{{\protect\ntt\bslash#1}}
%    LaTeX package name
\newcommand{\pkg}[1]{{\protect\ntt#1}}
%    File name
\newcommand{\fn}[1]{{\protect\ntt#1}}
%    environment name
\newcommand{\env}[1]{{\protect\ntt#1}}
\hfuzz1pc % Don't bother to report overfull boxes if overage is < 1pc

%       Theorem environments

%% \theoremstyle{plain} %% This is the default
\newtheorem[{}\it]{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Property}
\newtheorem{propo}[thm]{Proposition}
\newtheorem{ax}{Axiom}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem*[{}\it]{notation}{Notation}
\newtheorem{step}{Step}

\numberwithin{equation}{section}

\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\secref}[1]{\S\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}


%       Math definitions

\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\st}{\sigma}
\newcommand{\XcY}{{(X,Y)}}
\newcommand{\SX}{{S_X}}
\newcommand{\SY}{{S_Y}}
\newcommand{\SXY}{{S_{X,Y}}}
\newcommand{\SXgYy}{{S_{X|Y}(y)}}
\newcommand{\Cw}[1]{{\hat C_#1(X|Y)}}
\newcommand{\G}{{G(X|Y)}}
\newcommand{\PY}{{P_{\mathcal{Y}}}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\wt}{\widetilde}
\newcommand{\wh}{\widehat}
\newcommand{\dig}{\psi_0}
\newcommand{\trig}{\psi_1}
\newcommand{\imagj}{\mathrm{j}\mkern1mu} % Imaginary unit but with 100% more engineering
\newcommand{\ts}{t_\textnormal{s}}

\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\theta}{\vartheta}
\renewcommand{\kappa}{\varkappa}
\renewcommand{\rho}{\varrho}
\renewcommand{\phi}{\varphi}

\newcommand{\like}{\mathcal{L}} % likelihood
\newcommand{\loglike}{\ell} % log-likelihood
\newcommand{\e}{\mathrm{e}} % euler's constant
\newcommand{\pdf}{f} % probability density function
\newcommand{\htheta}{\hat{\theta}} % estimator (abuse of notation)
\newcommand{\hTheta}{\wh{\Theta}} % estimator
\DeclareMathOperator{\newdiff}{d} % use \dif instead
\newcommand{\dif}{\newdiff\!} % differential operator
\newcommand{\fisher}{\mathcal{I}} % fisher information matrix
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\plim}{plim}

\makeatletter
\DeclareRobustCommand{\expe}{\mathbb{E}\@ifstar\@firstofone\@expe}
\newcommand{\@expe}[1]{\left[#1\right]}
\makeatother

\makeatletter
\DeclareRobustCommand{\var}{\mathbb{V}\@ifstar\@firstofone\@expe}
\newcommand{\@var}[1]{\left[#1\right]}
\makeatother

%    \interval is used to provide better spacing after a [ that
%    is used as a closing delimiter.
\newcommand{\interval}[1]{\mathinner{#1}}

%    Notation for an expression evaluated at a particular condition. The
%    optional argument can be used to override automatic sizing of the
%    right vert bar, e.g. \eval[\biggr]{...}_{...}
\newcommand{\eval}[2][\right]{\relax
  \ifx#1\right\relax \left.\fi#2#1\rvert}

%    Enclose the argument in vert-bar delimiters:
\newcommand{\envert}[1]{\left\lvert#1\right\rvert}
\let\abs=\envert

%    Enclose the argument in double-vert-bar delimiters:
\newcommand{\enVert}[1]{\left\lVert#1\right\rVert}
\let\norm=\enVert

%\setcounter{tocdepth}{5}

\title[Fish schools tracking]{LINMA1731 -- Project 2019\\
Fish schools tracking}

\author{Louis Navarre}
\address{Université catholique de Louvain, Ottignies-Louvain-la-Neuve, Belgium}
\fulladdress{École Polytechnique, Université catholique de Louvain, Place de l'Université 1, 1348 Ottignies-Louvain-la-Neuve, Belgique}
\email{navarre.louis@student.uclouvain.be}
\givenname{Louis}
\surname{Navarre}

\author{Gilles Peiffer}
\address{Université catholique de Louvain, Ottignies-Louvain-la-Neuve, Belgium}
\fulladdress{École Polytechnique\\
	Université catholique de Louvain\\
	Place de l'Université 1, 1348 Ottignies-Louvain-la-Neuve, Belgium}
\email{gilles.peiffer@student.uclouvain.be}
\givenname{Gilles}
\surname{Peiffer}

%\oldsubsections
\copyrightnote{\textcopyright~2019 Gilles Peiffer and Louis Navarre}

\begin{document}

\begin{abstract}
	In this paper we solve the first part of the project for the class ``Stochastic processes: Estimation and prediction'' given during the Fall term of 2019 at the EPL by Luc Vandendorpe and Pierre-Antoine Absil.
	The average speed of each fish in a school of fish is approximated by a gamma-distributed random variable with a shape parameter \(k\) and a scale parameter \(s\), and various methods for estimating this quantity are given; numerical simulations are also included, using Julia and J\textsuperscript{u}MP~\cite{DunningHuchetteLubin2017}.
\end{abstract}

\maketitle
\tableofcontents
\newpage

\part{Average speed estimation}
\section{Introduction}
For the purpose of this project, we assume that the speed of each fish in a school
at time \(i\) is a random variable \(V_i\) following a Gamma distribution, as suggested in~\cite{huth:sim}.
This distribution is characterized by two parameters:
a shape parameter \(k > 0\) and a scale parameter \(s > 0\).
The parameters are the same for every fish and are time invariant.
The aim of this first part is to identify these two parameters using empirical observations \(v_i\).

\section{A maximum likelihood estimator for the scale parameter}
\label{sec:s_est}
Let \(v_i\) be i.i.d. realisations of a random variable following a Gamma distribution \(\Gamma(k, s)\) (with \(i = 1,\ldots, N)\)~\cite{wiki:gamma}.
We first assume that the shape parameter \(k\) is known.

We start by deriving the maximum likelihood estimator of \(\theta \coloneqq s\) based on \(N\) observations~\cite{wiki:mle}.
Since the estimand $\theta$ is a deterministic quantity, we use Fisher estimation.
In order to do this, let us restate the probability density function of \(V_i \sim \Gamma(k, s)\):
\begin{equation}
\pdf_{V_i}(v_i; k, s) = \frac{1}{\Gamma(k) s^k} v_i^{k-1} \e^{-\frac{v_i}{s}}\,, \quad i = 1, \ldots, N\,.\label{eq:gamma}
\end{equation}
With this in mind, we can find that the likelihood \(\like(v_1, \ldots, v_N; k, \theta)\) is given by
\begin{align}
\like(v_1, \ldots, v_N; k, \theta) &= \prod_{i=1}^{N} \pdf_{V_i}(v_i; k,\theta) = \prod_{i=1}^{N} \frac{1}{\Gamma(k) \theta^k} v_i^{k-1} \e^{-\frac{v_i}{\theta}}\,.
\end{align}
In order to alleviate notation, we compute instead the log-likelihood, which is generally easier to work with\footnote{This is possible because the values of \(\theta\) which maximize the log-likelihood also maximize the likelihood.}:
\begin{align}
\ell(v_1, \ldots, v_N; k, \theta) &\coloneqq \ln \like(v_1, \ldots, v_N; k, \theta)\\
&= \sum_{i=1}^{N} \ln\Bigg(\frac{1}{\Gamma(k) \theta^k} v_i^{k-1} \e^{-\frac{v_i}{\theta}}\Bigg)\\
&= (k-1) \sum_{i=1}^{N}\ln v_i - \sum_{i=1}^{N} \frac{v_i}{\theta} - N \big(k \ln \theta + \ln \Gamma(k)\big)\,.\label{eq:loglikelihood}
\end{align}
Now, in order to obtain the maximum likelihood estimate \(\htheta\), we must differentiate the log-likelihood with respect to the estimand \(\theta\), and set it equal to zero:
\begin{align}
\eval{\frac{\partial \ell(v_1, \ldots, v_N; k, \theta)}{\partial \theta}}_{\theta = \htheta} &= -\frac{kN}{\htheta} + \frac{\sum_{i=1}^{N} v_i}{\htheta^2} = 0\\
\iff \htheta &= \frac{\sum_{i=1}^{N} v_i}{kN} = \frac{\widebar{v}}{k}\,.\\
\intertext{This then allows us to find the maximum likelihood estimator \(\hTheta\), given by}
\hTheta &= \frac{\sum_{i=1}^{N} V_i}{kN} = \frac{\widebar{V}}{k}\,.\label{eq:mlestimator}
\end{align}

\section{Asymptotic properties of the maximum likelihood estimator}
\label{sec:prop}
We now wish to show some of the properties of this estimator.
The definitions of these properties are given in Appendix~\ref{app:defs}.
\begin{prop}
	The maximum likelihood estimator derived in \eqref{eq:mlestimator} is asymptotically unbiased, that is,
	\begin{equation}
	\lim_{N \to +\infty} \expe{g(V_1, \ldots, V_N); \theta} = \theta\,.
	\end{equation}
\end{prop}
\begin{proof}
	We wish to prove that \(\lim_{N \to +\infty} \expe{\frac{\widebar{V}}{k}} = \theta\).
	We recall that \(\expe{V_i} = k \theta\) for \(V_i \sim \Gamma(k, \theta)\)
	and that the expected value operator is linear to obtain
	\begin{equation}
	\expe{\frac{\widebar{V}}{k}} = \frac{\expe{\frac{1}{N} \sum_{i=1}^N V_i}}{k} = \frac{\frac{1}{N} \sum_{i=1}^N \expe{V_i}}{k} = \frac{\frac{1}{N} N k \theta}{k} = \theta\,.
	\end{equation}
	This proves that the maximum likelihood estimator of \eqref{eq:mlestimator} is unbiased,
	hence it is also asymptotically unbiased.
	This can be seen on Figures~\ref{fig:k_est}~and~\ref{fig:s_est}.
\end{proof}

\begin{prop}
	\label{prop:eff}
	The maximum likelihood estimator derived in \eqref{eq:mlestimator} is efficient.
\end{prop}
\begin{proof}
	We use the fact that the random variables are independent to simplify the computations.
	Since \(\theta\) is a scalar parameter, the Fisher information matrix is a scalar, equal to
	\begin{align}
	\fisher(\theta) &= - N \expe{\frac{\partial^2}{\partial \theta^2} \Bigg((k-1) \ln v_1 - \frac{v_1}{\theta} - \big(k \ln \theta + \ln \Gamma(k)\big)\Bigg)}\\
	&=  N\expe{\frac{\partial^2}{\partial \theta^2} \Big(\frac{v_1}{\theta} + k \ln \theta \Big)} = \frac{kN}{\theta^2}\,.\label{eq:crlb}
	\end{align}
	We must also compute the variance of the ML estimator \(\hTheta\), which is given by
	\begin{equation}
	\var{\hTheta} = \var{\frac{\overline{V}}{k}} = \frac{\theta^2}{kN}\,.
	\end{equation}
	The Cramér--Rao lower bound is thus reached for all values of \(\theta\), which concludes the proof.
	This is shown on Figure~\ref{fig:CRLB}.
\end{proof}

\begin{prop}
	The maximum likelihood estimator of \eqref{eq:mlestimator} is best asymptotically normal.
\end{prop}
\begin{proof}
	In our case, we can show using the Cramér--Rao lower bound that \(\Sigma\) is minimal if it is equal to \(\fisher^{-1}(\theta)\).
	To alleviate notations, we will write \(\ell(\theta)\) instead of \(\ell(v_1, \ldots, v_N; k, \theta)\).
	By definition, since \(\htheta = \argmax_{\theta} \ell(\theta)\),
	we know that \(\ell'(\htheta) = 0\).
	Let \(\theta_0\) be the true value of the parameter \(\theta\).
	We can then use Taylor expansion on \(\ell'(\htheta)\) around \(\htheta = \theta_0\) to obtain
	\begin{align}
	\ell'(\htheta) &= \ell'(\theta_0) + \frac{\ell''(\theta_0)}{1!} (\htheta - \theta_0) + \mathcal{O}\left((\htheta-\theta_0)^2\right)\,.
	\intertext{We know the expression on the left is zero, hence}
	\ell'(\theta_0) &= -\ell''(\theta_0) (\htheta - \theta_0) + \mathcal{O}\left((\htheta-\theta_0)^2\right)\,.
	\intertext{Rearranging and multiplying by \(\sqrt{n}\), we get}
	\sqrt{n}(\htheta - \theta_0) &= \frac{\ell'(\theta_0)/\sqrt{n}}{-\ell''(\theta_0)/n + \mathcal{O}\left((\htheta-\theta_0)/n\right)}\,.
	\end{align}
	
	Next, we need to show that \(\frac{1}{\sqrt{n}} \ell'(\theta_0) \sim \mathcal{N}\big(0, \fisher(\theta_0)\big)\).
	This is done using the Lindeberg--Lévy central limit theorem, in Appendix~\ref{app:banproof}~\cite{wiki:clt}.
	We know that \(\frac{1}{N} \ell''(\theta_0) = \fisher(\theta_0)\).
	Finally, we can rewrite
	\begin{equation}
	\sqrt{N} (\htheta - \theta_0) \sim \frac{\mathcal{N}\big(0, \fisher(\theta_0)\big)}{\fisher(\theta_0)} = \mathcal{N}\big(0, \fisher^{-1}(\theta_0)\big)\,,
	\end{equation}
	where we didn't take into account the remainder of the Taylor series, which goes to zero.
	This proves that the ML estimator is best asymptotically normal.
\end{proof}

\begin{prop}
	The maximum likelihood estimator of \eqref{eq:mlestimator} is consistent.
\end{prop}
\begin{proof}
	We have shown that the estimator is unbiased, hence its MSE is equal to its variance.
	Since the estimator is efficient by Property~\ref{prop:eff}, we know that its variance is equal to the Cramér--Rao lower bound, \(\cov \hTheta = \fisher^{-1}(\theta)\).
	We found this lower bound to be equal to \(\frac{\theta^2}{kN}\) in \eqref{eq:crlb}.
	We have
	\begin{equation}
	\lim_{N \to +\infty} \cov \hTheta = \lim_{N \to +\infty} \frac{\theta^2}{kN} = 0\,.
	\end{equation}
	This proves that the variance (and hence the mean square error) of the estimator goes to zero as \(N\) goes to infinity, hence the estimator is consistent.
\end{proof}

\section{Joint maximum likelihood estimation}
\label{sec:joint}
We now consider \(V_i \sim \Gamma(k, s)\) (for \(i = 1,\ldots,N)\) with both \(k\) and \(s\) unknown.
Before, we assumed \(k\) was known, so we could maximize the log-likelihood function with respect to \(s\).
Now, we have to maximize this function with respect to \(s\) and \(k\) at the same time.
We know the maximum likelihood estimator of \(s\), \(\hat{s} = f(k)\).
Therefore, in the log-likelihood function, we can replace all the occurrences of \(s\) by the estimator we found, \(\hat{s}\).
One then gets a function of \(k\) only, which can be differentiated and its derivative set to zero.
Solving this, one can find the maximum likelihood estimator of \(k\).
We abusively write \(\ell(\theta)\) instead of \(\ell(v_1, \ldots, v_N; \theta)\).
Substituting in the estimator \(\hat{s}\) instead of \(\theta\) in the log-likelihood given in \eqref{eq:loglikelihood}, one finds
\begin{align}
\ell(\theta) = (k-1) \sum_{i=1}^{N}\ln v_i - \sum_{i=1}^{N} \frac{kv_i}{\widebar{v}} - N k \ln \widebar{v} + N k \ln k - N \ln \Gamma(k)\big)\,.
\end{align}
Taking the derivative of this function with respect to \(k\), we get
\begin{align}
\eval{\frac{\partial \ell(\theta)}{\partial k}}_{k = \hat{k}} &= \sum_{i=1}^N \ln v_i - N - N \ln \widebar{v} + N \ln \hat{k} + \frac{N\hat{k}}{\hat{k}} - N\frac{\Gamma(\hat{k})}{\Gamma(\hat{k})}\dig(\hat{k})\\
&= \sum_{i=1}^N \ln v_i - N \ln \sum_{i=1}^N v_i + N \ln \hat{k}  + N \ln N - N \dig(\hat{k})\,,
\end{align}
where \(\dig\) is the digamma function, i.e. the logarithmic derivative of the gamma function~\cite{wolf:dig}.
One must now look for a root of this equation:
\begin{align}
\ln \hat{k} - \dig(\hat{k}) &= \ln \left(\sum_{i=1}^N v_i\right) - \ln N - \frac{\sum_{i=1}^N \ln v_i}{N}\\
\iff \ln \hat{k}  - \dig(\hat{k}) &= \ln \left(\frac{\sum_{i=1}^N v_i}{N}\right) - \frac{\sum_{i=1}^N \ln v_i}{N}\,.\label{eq:mlk}
\end{align}
This equation has no closed-form solution for \(\hat{k}\), but can be approximated using numerical methods since the function is very well-behaved.

\section{Numerical experiments with various estimators}
\label{sec:est_code}
For the numerical simulation, \(N\) random variables were generated from a distribution with parameters \(\Gamma(1, 2)\), for different values of \(N\) (\mintinline{julia}{10:50:1000}).
For each value of \(N\), the experiment was repeated \(M = 500\) times.

In order to use method of moments estimation for the Gamma distribution given in \eqref{eq:gamma},
one first needs to know its characteristic function: \(\phi_{V_i}(t) = \expe{\e^{\imagj t V_i}} = (1 - \imagj s t)^{-k}\).
The \(n\)th moment is given by \(\mu_n = \expe{V_i^n} = \imagj^{-n} \phi_{V_i}^{(n)}(0)\)~\cite{wiki:charfun}.
Since the parameter vector has dimension two,
we need to compute the first two moments.
These are given by \(\mu_1 = ks\) and \(\mu_2 = k(k+1) s^2\).
One can use the sample moments \(\hat{\mu}_1 = \frac{1}{N} \sum_{i=1}^N v_i\) and \(\hat{\mu}_2 = \frac{1}{N} \sum_{i=1}^N v_i^2\) to estimate \(\mu_1\) and \(\mu_2\).
Using some simple algebra, one then finds
\begin{equation}
\hat{k}_{\textnormal{MOM}} = \frac{\hat{\mu}_1^2}{\hat{\mu}_2 - \hat{\mu}_1^2}\,, \quad \hat{s}_{\textnormal{MOM}} = \frac{\hat{\mu}_2}{\hat{\mu}_1} - \hat{\mu}_1\,.
\end{equation}

The maximum likelihood estimators are computed using the formulas in \eqref{eq:mlestimator} and \eqref{eq:mlk}, for the given sample.
As mentioned in Section~\ref{sec:joint},
the maximum likelihood estimator for \(k\) has no closed-form solution,
but can be approximated using numerical methods which require an initial guess.
One such first guess could be provided by the method of moments estimator for the parameter,
that is
\begin{equation}
\hat{k}_{\textnormal{ML}}^{(0)} = \hat{k}_{\textnormal{MOM}} =  \frac{\hat{\mu}_1^2}{\hat{\mu}_2 - \hat{\mu}_1^2}\,.
\end{equation}
Another possible choice for the first guess is
\begin{equation}
\hat{k}_{\textnormal{ML}}^{(0)} = \frac{3 - \xi + \sqrt{(\xi - 3)^2 + 24\xi}}{12 \xi}\,, \quad \textnormal{where } \xi = \ln \widebar{v} + \frac{1}{N} \sum_{i=1}^N \ln v_i\,.
\end{equation}
This guess can be shown to be within \(\SI{1.5}{\percent}\) of the actual value~\cite{minka:gamma}.
The estimator for \(s\) can then be found from \eqref{eq:mlestimator}, using \(\hat{k}_{\textnormal{ML}}\) instead of \(k\).
\begin{figure}[!htbp]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\scalebox{0.75}{\input{img/k.tex}}
		\caption{}
		\label{fig:k_est}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\scalebox{0.75}{\input{img/s.tex}}
		\caption{}
		\label{fig:s_est}
	\end{subfigure}
	\caption{Both the method of moments estimator and the maximum likelihood estimator get increasingly accurate as the sample size goes up.
	The green line is the true value of the parameter, while the dots and squares indicate the mean of the estimators for a given value of \(N\).
	The error bars are determined by the standard deviation of the estimators.}
	\label{fig:est}
\end{figure}

On Figures~\ref{fig:k_est} and \ref{fig:s_est}, the mean and standard deviation are shown for the \(M\) values of both the method of moments estimator and the maximum likelihood estimator, for different values of the size of the sample, \(N\).
For both parameters, both estimators are unbiased, but the ML estimator has a lower variance.
This should not come as a surprise: since it is efficient, every other estimator must have a greater or equal asymptotic variance.
As is shown numerically in Section~\ref{sec:ratio},
this variance asymptotically goes to \(\fisher^{-1}(\theta)\),
the Cramér--Rao lower bound.
This is also expected, in light of Property~\ref{prop:eff}.\footnote{While Property~\ref{prop:eff} is only proved for the estimator in \eqref{eq:mlestimator}, it also holds for the estimator found in \eqref{eq:mlk}, as it is a general property of maximum likelihood estimators.}

\section{An analytical derivation of the Fisher information matrix}
\label{sec:fisher}
One can analytically derive the Fisher information matrix.
Since there are two estimators, the dimensions of the matrix are \(2 \times 2\).
Using the definition of the Fisher information matrix given in Theorem~\ref{theo:CR}, the Fisher matrix is given by
\begin{equation}
\label{fisher_matrix}
\fisher(\theta) = \begin{pmatrix}
-\expe{\dfrac{\partial^2 \ell(v; k, s)}{\partial s^2}} & -\expe{\dfrac{\partial^2 \ell(v; k, s)}{\partial k \partial s}} \\
-\expe{\dfrac{\partial^2 \ell(v; k, s)}{\partial s \partial k}} & -\expe{\dfrac{\partial^2 \ell(v; k, s)}{\partial k^2}} \end{pmatrix}\,,
\end{equation}
where \(\ell(v; k, s)\) is used as a shorthand for \(\ell(v_1, \ldots, v_N; k, s)\), given by
\begin{equation}
\ell(v_1, \ldots, v_N; k, s) = (k-1) \sum_{i=1}^{N}\ln v_i - \sum_{i=1}^{N} \frac{v_i}{s} - N \big(k \ln s + \ln \Gamma(k)\big)\,.
\end{equation}
Computing the entries of the matrix is fairly tedious, and the details are in Appendix~\ref{app:fisher_information}.
The Fisher information matrix is then
\begin{equation}
\fisher(\theta) = N \begin{pmatrix}
\frac{k}{s^2} & \frac{1}{s} \\
\frac{1}{s} & \trig(k) \end{pmatrix}\,,
\end{equation} 
and the Cramér--Rao lower bound, given by the inverse of the Fisher information matrix, is
\begin{equation}
\fisher^{-1}(\theta) = \frac{1}{N} \begin{pmatrix}
\trig(k) & -\frac{1}{s}\\
-\frac{1}{s} & \frac{k}{s^2}
\end{pmatrix} \frac{s^2}{k\trig(k) - 1}\,,\label{eq:fisher}
\end{equation}
where \(\trig\) is the trigamma function, i.e. the second derivative of the logarithm of the gamma function~\cite{wolf:trig}.

\section{Numerical evidence of convergence to the Cramér--Rao bound}
\label{sec:ratio}
Figure~\ref{fig:CRLB} gives the spectral norm as defined in~\cite{wiki:spectral} of
\begin{equation}
\mathscr{R} = \cov \hTheta \oslash \fisher(\theta) - \begin{pmatrix} 1&1\\1&1 \end{pmatrix}\,,
\end{equation}
where \(\oslash\) denotes Hadamard division as defined in~\cite{wiki:hadamard} and \(\hTheta\) is \((\hat{k}_{\textnormal{ML}}, \hat{s}_{\textnormal{ML}})\), for different values of \(N\).
If this estimator is efficient, we know by Definition~\ref{def_eff} that its covariance should asymptotically go to the inverse of the Fisher information matrix.

Let the covariance matrix be defined as in~\cite{wiki:scm}, that is
\begin{equation}
\cov \hTheta = \frac{1}{M-1} \begin{pmatrix} \sum_{i=1}^M \hat{s}_{i, \textnormal{c}}^2 & \sum_{i=1}^M \hat{k}_{i, \textnormal{c}} \hat{s}_{i, \textnormal{c}} \\ \sum_{i=1}^M \hat{s}_{i, \textnormal{c}} \hat{k}_{i, \textnormal{c}} & \sum_{i=1}^M \hat{k}_{i, \textnormal{c}}^2 \end{pmatrix}\,.
\end{equation}
where \(\hat{k}_i\) and \(\hat{s}_i\) are the maximum likelihood estimators for the sample in the \(i\)th repetition of the experiment, and \(\hat{\rho}_{i, \textnormal{c}} = \hat{\rho}_i - \tilde{\rho}\), with \(\tilde{\rho}\) denoting the mean over all experiments of \(\hat{\rho}\) for a given value of \(N\), for \(\rho = k, s\).

If one takes the Hadamard division of the covariance matrix by the inverse of the Fisher information matrix, the result should converge to a matrix of ones~\cite{wiki:hadamard}.
In order to visualize this convergence, the matrix must be ``centered'', by removing one in every position\footnote{If the matrix is not centered so as to make its asymptotic value equal to the zero matrix, then the norm does not suffice to prove convergence to a matrix of ones.}.

For any norm\footnote{For this paper, the spectral norm is used, but any norm would give similar results.} of the resulting matrix \(\mathscr{R}\), one can then observe that \(\lim_{N \to +\infty} \norm{\mathscr{R}} = 0\).
This is shown for the spectral norm on Figure~\ref{fig:CRLB}.
To generate this picture, estimators were computed for samples of size \(N \in \{\,10, 50, 150, 3000\,\}\).
This experiment was repeated \(M = 10000\) times.
The ratio matrix was then computed as the element-wise division between the empirical covariance of the estimators for a given \(N\) and the inverse of the Fisher information matrix, given by \eqref{eq:fisher}.
Finally, the matrix was centered, and its spectral norm was computed.

\begin{figure}[!htbp]
	\centering
	\scalebox{0.75}{\input{img/ratio.tex}}
	\caption{Using an adequately-centered spectral norm, one can visualize to which extent the Cramér--Rao lower bound is reached.
	The symbol ``\(\oslash\)'' denotes Hadamard division~\cite{wiki:hadamard}.}
	\label{fig:CRLB}
\end{figure}

\part{Particle filtering}

\begin{figure}[!htbp]
	\centering
	\scalebox{0.75}{\input{img/mse_np.tex}}
	\caption{}
\end{figure}

\section{Estimation of the parameters from noisy measurements}
Recall that the speed of the fish follows a Gamma distribution (\(V_i \sim \Gamma(k, s)\)).
Here, one gets noisy observations of the position of a fish at given instants in time.
It is rather straightforward to transform a noisy observation of the speed from these positions measurement, knowing the sampling period \(\ts\).
Once we have extracted a noisy approximation of the speed from this set of data, we can repeat what we explained earlier in the document, in Section~\ref{sec:est_code}, to estimate the parameters of the distribution.

We begin by defining the formula for the speed more rigorously: from \(x_p(i)\), the noisy measurement of the position of fish \(p\) at time $t=\ts i$, one can get \(v_p(i)\), the noisy observation of the speed of that fish at that time, by
\begin{align}
	\bm{v}_p(i) &= \frac{\norm{\bm{x}_p(i+1) + \bm{n}_p - \big(\bm{x}_p(i) + \bm{n}_p\big)} }{\ts}\,, \quad \forall p\,,
\end{align}
with \(\bm{n}_p \sim \mathcal{N}(0, \sigma^2_{\textnormal{obs}})\) an additive white noise.
Using the estimation techniques of Section~\ref{sec:joint}, one can get estimators of the two parameters of the Gamma distribution of the speed, \(\hat{k}\) and \(\hat{s}\), based on the observed speeds.

\section{Conclusion}
In Section~\ref{sec:s_est}, we looked at maximum likelihood estimation for the scale parameter of a Gamma distribution, \(s\).
We found the result to be \(\hat{s}_{\textnormal{ML}} = \frac{\widebar{V}}{k}\) in \eqref{eq:mlestimator}.
We proved some asymptotic properties of this estimator in Section~\ref{sec:prop}, showing that it is asymptotically unbiased, efficient, best asymptotically normal and consistent.
After establishing this, in Section~\ref{sec:joint}, we looked into estimating both parameters of the Gamma distribution jointly, using our previous estimator.
This led us to find that \(\hat{k}_{\textnormal{ML}}\) is the solution of \eqref{eq:mlk}.
This estimator has no closed form, hence in Section~\ref{sec:est_code} we turned to numerical methods to estimate it; comparing the maximum likelihood estimators with the method of moments estimators showed that both are unbiased (for both parameters), with the former generally having lower variance, while the latter can be useful as an initial guess for the maximum likelihood estimator.
Section~\ref{sec:fisher}, together with Appendix~\ref{app:fisher_information}, details an analytical derivation of the bound given by the Cramér--Rao inequality of Theorem~\ref{theo:CR}.
Finally, in Section~\ref{sec:ratio}, empirical evidence of the efficiency of the maximum likelihood estimator was found.

\clearpage
\appendix

\section{Definitions of properties}
\label{app:defs}

\begin{defn}[Unbiased estimator]
	\label{def_unbiased}
	The Fisher estimator \(\hTheta = g(Z)\) of \(\theta\) is \emph{unbiased} if
	\begin{equation}
	m_{\hTheta; \theta} \coloneqq \expe{g(Z); \theta} = \theta\,, \quad \textnormal{for all } \theta\,.
	\end{equation}
\end{defn}

\begin{thm}[Cramér--Rao inequality]
	\label{theo:CR}
	If \(Z = (Z_1, \ldots, Z_N)^T\) with i.i.d. random variables \(Z_k\) and if its probability density function given by \(\pdf_Z(z; \theta) = \prod_{k=1}^{N} \pdf_{Z_k}(z_k; \theta)\) satisfies certain regularity conditions, then the covariance of any unbiased estimator \(\hTheta\) satisfies the \emph{Cramér--Rao inequality}
	\begin{equation}
	\cov \hTheta \succeq \fisher^{-1}(\theta)\,,
	\end{equation}
	where \(\fisher(\theta)\) is the \(p \times p\) \emph{Fisher information matrix},
	defined by
	\begin{equation}
	\big[\fisher(\theta)\big]_{i, j} \coloneqq -\expe{\frac{\partial^2 \ln \pdf_Z(z; \theta)}{\partial \theta_i \partial \theta_j}}\,.
	\label{information_matrix}
	\end{equation}
\end{thm}
\begin{defn}[Efficient estimator]
	\label{def_eff}
	An unbiased estimator is said to be \emph{efficient} if it reaches the Cramér--Rao bound for all values of \(\theta\), that is,
	\begin{equation}
	\cov \hTheta = \fisher^{-1}(\theta)\,, \quad \forall \theta\,.
	\end{equation}
\end{defn}

\begin{defn}[Best asymptotically normal estimator]
	\label{def_normal}
	A sequence of consistent estimators \(\{\hTheta_N(Z)\}_{N \in \mathbb{N}}\) of \(\theta\) is called \emph{best asymptotically normal} if
	\begin{equation}
	\sqrt{N} \left(\hTheta_N(Z) - \theta\right) \xrightarrow[N \to +\infty]{\mathcal{D}} \mathcal{N}(0, \Sigma)\,,
	\end{equation}
	for some minimal positive definite matrix \(\Sigma\).
\end{defn}

\begin{defn}[Consistent estimator]
	\label{def_consis}
	A sequence \(\{\hTheta_N(Z)\}_{N \in \mathbb{N}}\) of estimators of \(\theta\) is called \emph{consistent} if
	\begin{equation}
	\plim_{N \to +\infty} \hTheta_N(Z) = \theta\,.
	\end{equation}
	Theorem~9.1 of~\cite{wms} proves that this is equivalent to the statement that the MSE of the estimator converges to zero as \(N\) goes to infinity.
\end{defn}

\section{Omitted proofs}
The following is a proof of asymptotic normality for the estimator in \eqref{eq:mlestimator}, using the Lindeberg--Lévy formulation of the central limit theorem.
\begin{proof}
\label{app:banproof}
We want to prove that  \(\frac{1}{\sqrt{N}}\ell'(\theta_0) \sim \mathcal{N}\big(0, \fisher(\theta_0)\big)\).
We simplify notation by writing \(\pdf(v_i)\) instead of \(\pdf_{V_i}(v_i; k, \theta)\)
and using Euler's notation for partial derivatives~\cite{wiki:eulerdiff}.
First, we show that the expected value of \(\frac{1}{\sqrt{N}}\ell'(\theta_0)\) is zero.
\begin{align}
\expe{\frac{\ell'(\theta_0)}{\sqrt{N}}} &= \int_{-\infty}^{+\infty} \partial_{\theta_0} \left(\sum_{i=1}^N \frac{\ln \pdf(v_i)}{\sqrt{N}}\right) \pdf(v_i)\dif v_i\\
&= \frac{1}{\sqrt{N}} \sum_{i=1}^N \int_{-\infty}^{+\infty} \frac{\partial_\theta \pdf(v_i)}{\pdf(v_i)} \pdf(v_i)\dif v_i\\
&= \frac{1}{\sqrt{N}} \sum_{i=1}^N \int_{-\infty}^{+\infty} \frac{\partial_\theta \pdf(v_i)}{\pdf(v_i)} \pdf(v_i) \dif v_i\\
&= \frac{1}{\sqrt{N}} \sum_{i=1}^N \int_{-\infty}^{+\infty} \partial_\theta \pdf(v_i) \dif v_i\\
&= \frac{1}{\sqrt{N}} \sum_{i=1}^N \partial_\theta \int_{-\infty}^{+\infty} \pdf(v_i) \dif v_i\\
&= 0\,.
\end{align}

Next, we compute the variance of \(\frac{1}{N} \ell'(\theta_0)\) (for \(i = 1, \ldots, N\))
\begin{align}
\expe{\big(\partial_{\theta_0} \ln \pdf(v_i)\big)^2} &= \int_{-\infty}^{+\infty} \partial_{\theta_0} \ln \pdf(v_i) \frac{\partial_{\theta_0} \pdf(v_i)}{\pdf(v_i)} \pdf(v_i)\dif v_i\\
&= \int_{-\infty}^{+\infty} \partial_{\theta_0} \ln \pdf(v_i) \partial_{\theta_0} \pdf(v_i) \dif v_i\,.
\intertext{Using the product rule, we can then find}
&= \begin{multlined}[t]
- \int_{-\infty}^{+\infty} \partial_{\theta_0 \theta_0} \ln \pdf(v_i) \pdf(v_i) \dif v_i \\
+ \int_{-\infty}^{+\infty} \partial_{\theta_0} \big(\partial_{\theta_0} \ln \pdf(v_i) \pdf(v_i)\big) \dif v_i
\end{multlined}\\
&= - \expe{\partial_{\theta_0 \theta_0 } \ln \pdf(v_i)} + \partial_{\theta_0} \int_{-\infty}^{\infty} \frac{\partial_{\theta_0} \pdf(v_i)}{\pdf(v_i)} \pdf(v_i) \dif v_i\\
&= \fisher(\theta)\,,
\end{align}
where the last expression can be shown to be zero by a similar argument as the one used above for the expected value.
Knowing this, one easily finds
\begin{equation}
\var{\frac{\ell'(\theta_0)}{\sqrt{N}}} = \frac{1}{N} \var{\sum_{i=1}^N \partial_{\theta_0} \ln \pdf(v_i)} = \fisher(\theta_0)\,,
\end{equation}
since the random variables are i.i.d..
Using the Lindeberg--Lévy CLT, we thus have
\begin{equation}
\frac{\ell'(\theta_0)}{\sqrt{N}} \sim \mathcal{N}\big(0, \fisher(\theta_0)\big)\,.\qedhere
\end{equation}

\end{proof}

\section{Computation of the Fisher information matrix}
\label{app:fisher_information}
In order to derive an analytical expression for the Fisher information matrix, we start by computing various derivatives of the log-likelihood which will be needed later (writing \(\ell(v; k, s)\) instead of \(\ell(v_1, \ldots, v_N; k, s)\)):
\begin{align}
\frac{\partial \ell(v; k, s)}{\partial s} &= \frac{1}{s^2}\sum_{i=1}^{N} v_i - \frac{Nk}{s}\,,\\
\frac{\partial \ell(v; k, s)}{\partial k} &= \sum_{i=1}^{N} \ln v_i - N \ln s - N \dig(k)\,,\\
\frac{\partial^2\ell(v; k, s)}{\partial s^2} &= -\frac{2}{s^3}\sum_{i=1}^{N} v_i + \frac{Nk}{s^2}\,,\label{eq:00}\\
\frac{\partial^2 \ell(v; k, s)}{\partial k^2} &= -N\psi_1(k)\,,\label{eq:11}\\
\frac{\partial^2 \ell(v; k, s)}{\partial k\partial s} = \frac{\partial^2 \ell(v; k, s)}{\partial s\partial k}&= -\frac{N}{s}\,.
\end{align}
In order to find the entries of the Fisher matrix, one must take the expectation of the previous derivatives:
\begin{align}
\Big[\fisher(\theta)\Big]_{0, 0} &= -\expe{\frac{\partial^2 \ell(v; k, s)}{\partial s^2}} = - \expe{-\frac{2}{s^3}\sum_{i=1}^{N} v_i + \frac{Nk}{s^2}}\\
&= \frac{2}{s^3}N k s - \frac{Nk}{s^2} = \frac{Nk}{s^2}\,,\\
\Big[\fisher(\theta)\Big]_{0, 1} = \Big[\fisher(\theta)\Big]_{1, 0} &=  -\expe{\frac{\partial^2 \ell(v; k, s)}{\partial k \partial s}} = \frac{N}{s}\,,\\
\Big[\fisher(\theta)\Big]_{1, 1} &= -\expe{\frac{\partial^2 \ell(v; k, s)}{\partial k^2}} = -N \trig(k)\,.
\end{align}

\bibliography{report-linma1731-project}
\bibliographystyle{aomplain}

\end{document}
\endinput
