%%% ====================================================================
%%% @LaTeX-file{
%%%   filename  = "aomsample.tex",
%%%   copyright = "Copyright 1995, 1999 American Mathematical Society,
%%%                2005 Hebrew University Magnes Press,
%%%                all rights reserved.  Copying of this file is
%%%                authorized only if either:
%%%                (1) you make absolutely no changes to your copy,
%%%                including name; OR
%%%                (2) if you do make changes, you first rename it
%%%                to some other name.",
%%% }
%%% ====================================================================
\NeedsTeXFormat{LaTeX2e}% LaTeX 2.09 can't be used (nor non-LaTeX)
[1994/12/01]% LaTeX date must December 1994 or later
\documentclass[final]{aomart}
\usepackage[english]{babel}

\usepackage{mathtools,amssymb,amsthm, mathrsfs}
\usepackage{bm}
\usepackage{float}
\usepackage{siunitx}
\usepackage{minted}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepackage{subcaption}
\usepackage{url}

%    Some definitions useful in producing this sort of documentation:
\chardef\bslash=`\\ % p. 424, TeXbook
%    Normalized (nonbold, nonitalic) tt font, to avoid font
%    substitution warning messages if tt is used inside section
%    headings and other places where odd font combinations might
%    result.
\newcommand{\ntt}{\normalfont\ttfamily}
%    command name
\newcommand{\cn}[1]{{\protect\ntt\bslash#1}}
%    LaTeX package name
\newcommand{\pkg}[1]{{\protect\ntt#1}}
%    File name
\newcommand{\fn}[1]{{\protect\ntt#1}}
%    environment name
\newcommand{\env}[1]{{\protect\ntt#1}}
\hfuzz1pc % Don't bother to report overfull boxes if overage is < 1pc

%       Theorem environments

%% \theoremstyle{plain} %% This is the default
\newtheorem[{}\it]{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Property}
\newtheorem{propo}[thm]{Proposition}
\newtheorem{ax}{Axiom}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem*[{}\it]{notation}{Notation}
\newtheorem{step}{Step}

\numberwithin{equation}{section}

\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\secref}[1]{\S\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}


%       Math definitions

\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\st}{\sigma}
\newcommand{\XcY}{{(X,Y)}}
\newcommand{\SX}{{S_X}}
\newcommand{\SY}{{S_Y}}
\newcommand{\SXY}{{S_{X,Y}}}
\newcommand{\SXgYy}{{S_{X|Y}(y)}}
\newcommand{\Cw}[1]{{\hat C_#1(X|Y)}}
\newcommand{\G}{{G(X|Y)}}
\newcommand{\PY}{{P_{\mathcal{Y}}}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\wt}{\widetilde}
\newcommand{\wh}{\widehat}
\newcommand{\dig}{\psi_0}
\newcommand{\trig}{\psi_1}
\newcommand{\imagj}{\mathrm{j}\mkern1mu} % Imaginary unit but with 100% more engineering
\newcommand{\ts}{t_\textnormal{s}}
\newcommand{\np}{N_\textnormal{p}}
\newcommand{\sset}{\mathcal{S}}

\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\theta}{\vartheta}
\renewcommand{\kappa}{\varkappa}
\renewcommand{\rho}{\varrho}
\renewcommand{\phi}{\varphi}

\newcommand{\like}{\mathcal{L}} % likelihood
\newcommand{\loglike}{\ell} % log-likelihood
\newcommand{\e}{\mathrm{e}} % euler's constant
\newcommand{\pdf}{f} % probability density function
\newcommand{\htheta}{\hat{\theta}} % estimator (abuse of notation)
\newcommand{\hTheta}{\wh{\Theta}} % estimator
\DeclareMathOperator{\newdiff}{d} % use \dif instead
\newcommand{\dif}{\newdiff\!} % differential operator
\newcommand{\fisher}{\mathcal{I}} % fisher information matrix
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\plim}{plim}

\makeatletter
\DeclareRobustCommand{\expe}{\mathbb{E}\@ifstar\@firstofone\@expe}
\newcommand{\@expe}[1]{\left[#1\right]}
\makeatother

\makeatletter
\DeclareRobustCommand{\var}{\mathbb{V}\@ifstar\@firstofone\@expe}
\newcommand{\@var}[1]{\left[#1\right]}
\makeatother

%    \interval is used to provide better spacing after a [ that
%    is used as a closing delimiter.
\newcommand{\interval}[1]{\mathinner{#1}}

%    Notation for an expression evaluated at a particular condition. The
%    optional argument can be used to override automatic sizing of the
%    right vert bar, e.g. \eval[\biggr]{...}_{...}
\newcommand{\eval}[2][\right]{\relax
  \ifx#1\right\relax \left.\fi#2#1\rvert}

%    Enclose the argument in vert-bar delimiters:
\newcommand{\envert}[1]{\left\lvert#1\right\rvert}
\let\abs=\envert

%    Enclose the argument in double-vert-bar delimiters:
\newcommand{\enVert}[1]{\left\lVert#1\right\rVert}
\let\norm=\enVert

%\setcounter{tocdepth}{5}

\title[Fish schools tracking]{LINMA1731 -- Project 2019\\
Fish schools tracking}

\author{Louis Navarre}
\address{Université catholique de Louvain, Ottignies-Louvain-la-Neuve, Belgium}
\fulladdress{École Polytechnique, Université catholique de Louvain, Place de l'Université 1, 1348 Ottignies-Louvain-la-Neuve, Belgique}
\email{navarre.louis@student.uclouvain.be}
\givenname{Louis}
\surname{Navarre}

\author{Gilles Peiffer}
\address{Université catholique de Louvain, Ottignies-Louvain-la-Neuve, Belgium}
\fulladdress{École Polytechnique\\
	Université catholique de Louvain\\
	Place de l'Université 1, 1348 Ottignies-Louvain-la-Neuve, Belgium}
\email{gilles.peiffer@student.uclouvain.be}
\givenname{Gilles}
\surname{Peiffer}

%\oldsubsections
\copyrightnote{\textcopyright~2019 Gilles Peiffer and Louis Navarre}

\begin{document}

\begin{abstract}
	In this paper we propose a solution to the project for the class ``Stochastic processes: Estimation and prediction'' given during the Fall term of 2019 at the EPL by Luc Vandendorpe and Pierre-Antoine Absil.
	The average speed of each fish in a school of fish is approximated by a gamma-distributed random variable with a shape parameter \(k\) and a scale parameter \(s\), and various methods for estimating this quantity are given; numerical simulations are also included, using Julia and J\textsuperscript{u}MP~\cite{DunningHuchetteLubin2017}.
	This paper also details how to obtain these speed measurements from noisy observations of the position.
	Using a particle filter, one can then try to use these noisy observations in order to obtain a more accurate position for each of the fish.
	Finally, the effect of various parameters of the model, such as the number of particles per fish, the sampling period and the standard deviation of the observational noise is studied.
\end{abstract}

\maketitle
\tableofcontents
\newpage

\part{Average speed estimation}
\section{Introduction}
For the purpose of this project, we assume that the speed of each fish in a school
at time \(i\) is a random variable \(V_i\) following a Gamma distribution, as suggested in~\cite{huth:sim}.
This distribution is characterized by two parameters:
a shape parameter \(k > 0\) and a scale parameter \(s > 0\).
The parameters are the same for every fish and are time invariant.
The aim of this first part is to identify these two parameters using empirical observations \(v_i\).

\section{A maximum likelihood estimator for the scale parameter}
\label{sec:s_est}
Let \(v_i\) be i.i.d. realisations of a random variable following a Gamma distribution \(\Gamma(k, s)\) (with \(i = 1,\ldots, N)\)~\cite{wiki:gamma}.
We first assume that the shape parameter \(k\) is known.

We start by deriving the maximum likelihood estimator of \(\theta \coloneqq s\) based on \(N\) observations~\cite{wiki:mle}.
Since the estimand $\theta$ is a deterministic quantity, we use Fisher estimation.
In order to do this, let us restate the probability density function of \(V_i \sim \Gamma(k, s)\):
\begin{equation}
\pdf_{V_i}(v_i; k, s) = \frac{1}{\Gamma(k) s^k} v_i^{k-1} \e^{-\frac{v_i}{s}}\,, \quad i = 1, \ldots, N\,.\label{eq:gamma}
\end{equation}
With this in mind, we can find that the likelihood \(\like(v_1, \ldots, v_N; k, \theta)\) is given by
\begin{align}
\like(v_1, \ldots, v_N; k, \theta) &= \prod_{i=1}^{N} \pdf_{V_i}(v_i; k,\theta) = \prod_{i=1}^{N} \frac{1}{\Gamma(k) \theta^k} v_i^{k-1} \e^{-\frac{v_i}{\theta}}\,.
\end{align}
In order to alleviate notation, we compute instead the log-likelihood, which is generally easier to work with\footnote{This is possible because the values of \(\theta\) which maximize the log-likelihood also maximize the likelihood.}:
\begin{align}
\ell(v_1, \ldots, v_N; k, \theta) &\coloneqq \ln \like(v_1, \ldots, v_N; k, \theta)\\
&= \sum_{i=1}^{N} \ln\Bigg(\frac{1}{\Gamma(k) \theta^k} v_i^{k-1} \e^{-\frac{v_i}{\theta}}\Bigg)\\
&= (k-1) \sum_{i=1}^{N}\ln v_i - \sum_{i=1}^{N} \frac{v_i}{\theta} - N \big(k \ln \theta + \ln \Gamma(k)\big)\,.\label{eq:loglikelihood}
\end{align}
Now, in order to obtain the maximum likelihood estimate \(\htheta\), we must differentiate the log-likelihood with respect to the estimand \(\theta\), and set it equal to zero:
\begin{align}
\eval{\frac{\partial \ell(v_1, \ldots, v_N; k, \theta)}{\partial \theta}}_{\theta = \htheta} &= -\frac{kN}{\htheta} + \frac{\sum_{i=1}^{N} v_i}{\htheta^2} = 0\\
\iff \htheta &= \frac{\sum_{i=1}^{N} v_i}{kN} = \frac{\widebar{v}}{k}\,.\\
\intertext{This then allows us to find the maximum likelihood estimator \(\hTheta\), given by}
\hTheta &= \frac{\sum_{i=1}^{N} V_i}{kN} = \frac{\widebar{V}}{k}\,.\label{eq:mlestimator}
\end{align}

\section{Asymptotic properties of the maximum likelihood estimator}
\label{sec:prop}
We now wish to show some of the properties of this estimator.
The definitions of these properties are given in Appendix~\ref{app:defs}.
\begin{prop}
	The maximum likelihood estimator derived in \eqref{eq:mlestimator} is asymptotically unbiased, that is,
	\begin{equation}
	\lim_{N \to +\infty} \expe{g(V_1, \ldots, V_N); \theta} = \theta\,.
	\end{equation}
\end{prop}
\begin{proof}
	We wish to prove that \(\lim_{N \to +\infty} \expe{\frac{\widebar{V}}{k}} = \theta\).
	We recall that \(\expe{V_i} = k \theta\) for \(V_i \sim \Gamma(k, \theta)\)
	and that the expected value operator is linear to obtain
	\begin{equation}
	\expe{\frac{\widebar{V}}{k}} = \frac{\expe{\frac{1}{N} \sum_{i=1}^N V_i}}{k} = \frac{\frac{1}{N} \sum_{i=1}^N \expe{V_i}}{k} = \frac{\frac{1}{N} N k \theta}{k} = \theta\,.
	\end{equation}
	This proves that the maximum likelihood estimator of \eqref{eq:mlestimator} is unbiased,
	hence it is also asymptotically unbiased.
	This can be seen on Figures~\ref{fig:k_est}~and~\ref{fig:s_est}.
\end{proof}

\begin{prop}
	\label{prop:eff}
	The maximum likelihood estimator derived in \eqref{eq:mlestimator} is efficient.
\end{prop}
\begin{proof}
	We use the fact that the random variables are independent to simplify the computations.
	Since \(\theta\) is a scalar parameter, the Fisher information matrix is a scalar, equal to
	\begin{align}
	\fisher(\theta) &= - N \expe{\frac{\partial^2}{\partial \theta^2} \Bigg((k-1) \ln v_1 - \frac{v_1}{\theta} - \big(k \ln \theta + \ln \Gamma(k)\big)\Bigg)}\\
	&=  N\expe{\frac{\partial^2}{\partial \theta^2} \Big(\frac{v_1}{\theta} + k \ln \theta \Big)} = \frac{kN}{\theta^2}\,.\label{eq:crlb}
	\end{align}
	We must also compute the variance of the ML estimator \(\hTheta\), which is given by
	\begin{equation}
	\var{\hTheta} = \var{\frac{\overline{V}}{k}} = \frac{\theta^2}{kN}\,.
	\end{equation}
	The Cramér--Rao lower bound is thus reached for all values of \(\theta\), which concludes the proof.
	This is shown on Figure~\ref{fig:CRLB}.
\end{proof}

\begin{prop}
	The maximum likelihood estimator of \eqref{eq:mlestimator} is best asymptotically normal.
\end{prop}
\begin{proof}
	In our case, we can show using the Cramér--Rao lower bound that \(\Sigma\) is minimal if it is equal to \(\fisher^{-1}(\theta)\).
	To alleviate notations, we will write \(\ell(\theta)\) instead of \(\ell(v_1, \ldots, v_N; k, \theta)\).
	By definition, since \(\htheta = \argmax_{\theta} \ell(\theta)\),
	we know that \(\ell'(\htheta) = 0\).
	Let \(\theta_0\) be the true value of the parameter \(\theta\).
	We can then use Taylor expansion on \(\ell'(\htheta)\) around \(\htheta = \theta_0\) to obtain
	\begin{align}
	\ell'(\htheta) &= \ell'(\theta_0) + \frac{\ell''(\theta_0)}{1!} (\htheta - \theta_0) + \mathcal{O}\left((\htheta-\theta_0)^2\right)\,.
	\intertext{We know the expression on the left is zero, hence}
	\ell'(\theta_0) &= -\ell''(\theta_0) (\htheta - \theta_0) + \mathcal{O}\left((\htheta-\theta_0)^2\right)\,.
	\intertext{Rearranging and multiplying by \(\sqrt{n}\), we get}
	\sqrt{n}(\htheta - \theta_0) &= \frac{\ell'(\theta_0)/\sqrt{n}}{-\ell''(\theta_0)/n + \mathcal{O}\left((\htheta-\theta_0)/n\right)}\,.
	\end{align}
	
	Next, we need to show that \(\frac{1}{\sqrt{n}} \ell'(\theta_0) \sim \mathcal{N}\big(0, \fisher(\theta_0)\big)\).
	This is done using the Lindeberg--Lévy central limit theorem, in Appendix~\ref{app:banproof}~\cite{wiki:clt}.
	We know that \(\frac{1}{N} \ell''(\theta_0) = \fisher(\theta_0)\).
	Finally, we can rewrite
	\begin{equation}
	\sqrt{N} (\htheta - \theta_0) \sim \frac{\mathcal{N}\big(0, \fisher(\theta_0)\big)}{\fisher(\theta_0)} = \mathcal{N}\big(0, \fisher^{-1}(\theta_0)\big)\,,
	\end{equation}
	where we didn't take into account the remainder of the Taylor series, which goes to zero.
	This proves that the ML estimator is best asymptotically normal.
\end{proof}

\begin{prop}
	The maximum likelihood estimator of \eqref{eq:mlestimator} is consistent.
\end{prop}
\begin{proof}
	We have shown that the estimator is unbiased, hence its MSE is equal to its variance.
	Since the estimator is efficient by Property~\ref{prop:eff}, we know that its variance is equal to the Cramér--Rao lower bound, \(\cov \hTheta = \fisher^{-1}(\theta)\).
	We found this lower bound to be equal to \(\frac{\theta^2}{kN}\) in \eqref{eq:crlb}.
	We have
	\begin{equation}
	\lim_{N \to +\infty} \cov \hTheta = \lim_{N \to +\infty} \frac{\theta^2}{kN} = 0\,.
	\end{equation}
	This proves that the variance (and hence the mean square error) of the estimator goes to zero as \(N\) goes to infinity, hence the estimator is consistent.
\end{proof}

\section{Joint maximum likelihood estimation}
\label{sec:joint}
We now consider \(V_i \sim \Gamma(k, s)\) (for \(i = 1,\ldots,N)\) with both \(k\) and \(s\) unknown.
Before, we assumed \(k\) was known, so we could maximize the log-likelihood function with respect to \(s\).
Now, we have to maximize this function with respect to \(s\) and \(k\) at the same time.
We know the maximum likelihood estimator of \(s\), \(\hat{s} = f(k)\).
Therefore, in the log-likelihood function, we can replace all the occurrences of \(s\) by the estimator we found, \(\hat{s}\).
One then gets a function of \(k\) only, which can be differentiated and its derivative set to zero.
Solving this, one can find the maximum likelihood estimator of \(k\).
We abusively write \(\ell(\theta)\) instead of \(\ell(v_1, \ldots, v_N; \theta)\).
Substituting in the estimator \(\hat{s}\) instead of \(\theta\) in the log-likelihood given in \eqref{eq:loglikelihood}, one finds
\begin{align}
\ell(\theta) = (k-1) \sum_{i=1}^{N}\ln v_i - \sum_{i=1}^{N} \frac{kv_i}{\widebar{v}} - N k \ln \widebar{v} + N k \ln k - N \ln \Gamma(k)\big)\,.
\end{align}
Taking the derivative of this function with respect to \(k\), we get
\begin{align}
\eval{\frac{\partial \ell(\theta)}{\partial k}}_{k = \hat{k}} &= \sum_{i=1}^N \ln v_i - N - N \ln \widebar{v} + N \ln \hat{k} + \frac{N\hat{k}}{\hat{k}} - N\frac{\Gamma(\hat{k})}{\Gamma(\hat{k})}\dig(\hat{k})\\
&= \sum_{i=1}^N \ln v_i - N \ln \sum_{i=1}^N v_i + N \ln \hat{k}  + N \ln N - N \dig(\hat{k})\,,
\end{align}
where \(\dig\) is the digamma function, i.e. the logarithmic derivative of the gamma function~\cite{wolf:dig}.
One must now look for a root of this equation:
\begin{align}
\ln \hat{k} - \dig(\hat{k}) &= \ln \left(\sum_{i=1}^N v_i\right) - \ln N - \frac{\sum_{i=1}^N \ln v_i}{N}\\
\iff \ln \hat{k}  - \dig(\hat{k}) &= \ln \left(\frac{\sum_{i=1}^N v_i}{N}\right) - \frac{\sum_{i=1}^N \ln v_i}{N}\,.\label{eq:mlk}
\end{align}
This equation has no closed-form solution for \(\hat{k}\), but can be approximated using numerical methods since the function is very well-behaved.

\section{Numerical experiments with various estimators}
\label{sec:est_code}
For the numerical simulation, \(N\) random variables were generated from a distribution with parameters \(\Gamma(1, 2)\), for different values of \(N\) (\mintinline{julia}{10:50:1000}).
For each value of \(N\), the experiment was repeated \(M = 500\) times.

In order to use method of moments estimation for the Gamma distribution given in \eqref{eq:gamma},
one first needs to know its characteristic function: \(\phi_{V_i}(t) = \expe{\e^{\imagj t V_i}} = (1 - \imagj s t)^{-k}\).
The \(n\)th moment is given by \(\mu_n = \expe{V_i^n} = \imagj^{-n} \phi_{V_i}^{(n)}(0)\)~\cite{wiki:charfun}.
Since the parameter vector has dimension two,
we need to compute the first two moments.
These are given by \(\mu_1 = ks\) and \(\mu_2 = k(k+1) s^2\).
One can use the sample moments \(\hat{\mu}_1 = \frac{1}{N} \sum_{i=1}^N v_i\) and \(\hat{\mu}_2 = \frac{1}{N} \sum_{i=1}^N v_i^2\) to estimate \(\mu_1\) and \(\mu_2\).
Using some simple algebra, one then finds
\begin{equation}
\hat{k}_{\textnormal{MOM}} = \frac{\hat{\mu}_1^2}{\hat{\mu}_2 - \hat{\mu}_1^2}\,, \quad \hat{s}_{\textnormal{MOM}} = \frac{\hat{\mu}_2}{\hat{\mu}_1} - \hat{\mu}_1\,.
\end{equation}

The maximum likelihood estimators are computed using the formulas in \eqref{eq:mlestimator} and \eqref{eq:mlk}, for the given sample.
As mentioned in Section~\ref{sec:joint},
the maximum likelihood estimator for \(k\) has no closed-form solution,
but can be approximated using numerical methods which require an initial guess.
One such first guess could be provided by the method of moments estimator for the parameter,
that is
\begin{equation}
\hat{k}_{\textnormal{ML}}^{(0)} = \hat{k}_{\textnormal{MOM}} =  \frac{\hat{\mu}_1^2}{\hat{\mu}_2 - \hat{\mu}_1^2}\,.
\end{equation}
Another possible choice for the first guess is
\begin{equation}
\hat{k}_{\textnormal{ML}}^{(0)} = \frac{3 - \xi + \sqrt{(\xi - 3)^2 + 24\xi}}{12 \xi}\,, \quad \textnormal{where } \xi = \ln \widebar{v} + \frac{1}{N} \sum_{i=1}^N \ln v_i\,.
\end{equation}
This guess can be shown to be within \(\SI{1.5}{\percent}\) of the actual value~\cite{minka:gamma}.
The estimator for \(s\) can then be found from \eqref{eq:mlestimator}, using \(\hat{k}_{\textnormal{ML}}\) instead of \(k\).
\begin{figure}[!htbp]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\scalebox{0.75}{\input{img/k.tex}}
		\caption{}
		\label{fig:k_est}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\scalebox{0.75}{\input{img/s.tex}}
		\caption{}
		\label{fig:s_est}
	\end{subfigure}
	\caption{Both the method of moments estimator and the maximum likelihood estimator get increasingly accurate as the sample size goes up.
	The green line is the true value of the parameter, while the dots and squares indicate the mean of the estimators for a given value of \(N\).
	The error bars are determined by the standard deviation of the estimators.}
	\label{fig:est}
\end{figure}

On Figures~\ref{fig:k_est} and \ref{fig:s_est}, the mean and standard deviation are shown for the \(M\) values of both the method of moments estimator and the maximum likelihood estimator, for different values of the size of the sample, \(N\).
For both parameters, both estimators are unbiased, but the ML estimator has a lower variance.
This should not come as a surprise: since it is efficient, every other estimator must have a greater or equal asymptotic variance.
As is shown numerically in Section~\ref{sec:ratio},
this variance asymptotically goes to \(\fisher^{-1}(\theta)\),
the Cramér--Rao lower bound.
This is also expected, in light of Property~\ref{prop:eff}.\footnote{While Property~\ref{prop:eff} is only proved for the estimator in \eqref{eq:mlestimator}, it also holds for the estimator found in \eqref{eq:mlk}, as it is a general property of maximum likelihood estimators.}

\section{An analytical derivation of the Fisher information matrix}
\label{sec:fisher}
One can analytically derive the Fisher information matrix.
Since there are two estimators, the dimensions of the matrix are \(2 \times 2\).
Using the definition of the Fisher information matrix given in Theorem~\ref{theo:CR}, the Fisher matrix is given by
\begin{equation}
\label{fisher_matrix}
\fisher(\theta) = \begin{pmatrix}
-\expe{\dfrac{\partial^2 \ell(v; k, s)}{\partial s^2}} & -\expe{\dfrac{\partial^2 \ell(v; k, s)}{\partial k \partial s}} \\
-\expe{\dfrac{\partial^2 \ell(v; k, s)}{\partial s \partial k}} & -\expe{\dfrac{\partial^2 \ell(v; k, s)}{\partial k^2}} \end{pmatrix}\,,
\end{equation}
where \(\ell(v; k, s)\) is used as a shorthand for \(\ell(v_1, \ldots, v_N; k, s)\), given by
\begin{equation}
\ell(v_1, \ldots, v_N; k, s) = (k-1) \sum_{i=1}^{N}\ln v_i - \sum_{i=1}^{N} \frac{v_i}{s} - N \big(k \ln s + \ln \Gamma(k)\big)\,.
\end{equation}
Computing the entries of the matrix is fairly tedious, and the details are in Appendix~\ref{app:fisher_information}.
The Fisher information matrix is then
\begin{equation}
\fisher(\theta) = N \begin{pmatrix}
\frac{k}{s^2} & \frac{1}{s} \\
\frac{1}{s} & \trig(k) \end{pmatrix}\,,
\end{equation} 
and the Cramér--Rao lower bound, given by the inverse of the Fisher information matrix, is
\begin{equation}
\fisher^{-1}(\theta) = \frac{1}{N} \begin{pmatrix}
\trig(k) & -\frac{1}{s}\\
-\frac{1}{s} & \frac{k}{s^2}
\end{pmatrix} \frac{s^2}{k\trig(k) - 1}\,,\label{eq:fisher}
\end{equation}
where \(\trig\) is the trigamma function, i.e. the second derivative of the logarithm of the gamma function~\cite{wolf:trig}.

\section{Numerical evidence of convergence to the Cramér--Rao bound}
\label{sec:ratio}
Figure~\ref{fig:CRLB} gives the spectral norm as defined in~\cite{wiki:spectral} of
\begin{equation}
\mathscr{R} = \cov \hTheta \oslash \fisher(\theta) - \begin{pmatrix} 1&1\\1&1 \end{pmatrix}\,,
\end{equation}
where \(\oslash\) denotes Hadamard division as defined in~\cite{wiki:hadamard} and \(\hTheta\) is \((\hat{k}_{\textnormal{ML}}, \hat{s}_{\textnormal{ML}})\), for different values of \(N\).
If this estimator is efficient, we know by Definition~\ref{def_eff} that its covariance should asymptotically go to the inverse of the Fisher information matrix.

Let the covariance matrix be defined as in~\cite{wiki:scm}, that is
\begin{equation}
\cov \hTheta = \frac{1}{M-1} \begin{pmatrix} \sum_{i=1}^M \hat{s}_{i, \textnormal{c}}^2 & \sum_{i=1}^M \hat{k}_{i, \textnormal{c}} \hat{s}_{i, \textnormal{c}} \\ \sum_{i=1}^M \hat{s}_{i, \textnormal{c}} \hat{k}_{i, \textnormal{c}} & \sum_{i=1}^M \hat{k}_{i, \textnormal{c}}^2 \end{pmatrix}\,.
\end{equation}
where \(\hat{k}_i\) and \(\hat{s}_i\) are the maximum likelihood estimators for the sample in the \(i\)th repetition of the experiment, and \(\hat{\rho}_{i, \textnormal{c}} = \hat{\rho}_i - \tilde{\rho}\), with \(\tilde{\rho}\) denoting the mean over all experiments of \(\hat{\rho}\) for a given value of \(N\), for \(\rho = k, s\).

If one takes the Hadamard division of the covariance matrix by the inverse of the Fisher information matrix, the result should converge to a matrix of ones~\cite{wiki:hadamard}.
In order to visualize this convergence, the matrix must be ``centered'', by removing one in every position\footnote{If the matrix is not centered so as to make its asymptotic value equal to the zero matrix, then the norm does not suffice to prove convergence to a matrix of ones.}.

For any norm\footnote{For this paper, the spectral norm is used, but any norm would give similar results.} of the resulting matrix \(\mathscr{R}\), one can then observe that \(\lim_{N \to +\infty} \norm{\mathscr{R}} = 0\).
This is shown for the spectral norm on Figure~\ref{fig:CRLB}.
To generate this picture, estimators were computed for samples of size \(N \in \{\,10, 50, 150, 3000\,\}\).
This experiment was repeated \(M = 10000\) times.
The ratio matrix was then computed as the element-wise division between the empirical covariance of the estimators for a given \(N\) and the inverse of the Fisher information matrix, given by \eqref{eq:fisher}.
Finally, the matrix was centered, and its spectral norm was computed.

\begin{figure}[!htbp]
	\centering
	\scalebox{0.75}{\input{img/ratio.tex}}
	\caption{Using an adequately-centered spectral norm, one can visualize to which extent the Cramér--Rao lower bound is reached.
	The symbol ``\(\oslash\)'' denotes Hadamard division~\cite{wiki:hadamard}.}
	\label{fig:CRLB}
\end{figure}

\part{Particle filtering}

\section{Introduction}
For the second part of the project, the task is to use noisy observations of the position of a fish in order to compute the parameters of the Gamma distribution governing its speed.
With these parameters, a simulation is then completed which models the reaction and movement of the fish to a nearby predator.
More precisely, a sequential Monte Carlo method is used to extract the real positions from the noisy measurements.
In order to do this, one can play with various parameters, such as the number of fish, the number of particles per fish in the simulation, the time step between observations or the standard deviation of the noise which is added to the position to obtain the observations.

\section{Estimation of the parameters from noisy measurements}
\label{sec:speed}
Recall that the speed of the fish follows a Gamma distribution (\(V_i \sim \Gamma(k, s)\)).
Here, one gets noisy observations of the position of a fish at given instants in time.
It is rather straightforward to transform a noisy observation of the speed from these positions measurement, knowing the sampling period \(\ts\).
Once we have extracted a noisy approximation of the speed from this set of data, we can repeat what we explained earlier in the document, in Section~\ref{sec:est_code}, to estimate the parameters of the distribution.

We begin by defining the formula for the speed more rigorously: from \(x_p(i)\), the noisy measurement of the position of fish \(p\) at step $i$, one can get \(v_p(i)\), the noisy observation of the speed of that fish at that step, by
\begin{align}
	\bm{v}_p(i) &= \frac{\norm{\bm{x}_p(i+1) + \bm{n}_p(i+1) - \big(\bm{x}_p(i) + \bm{n}_p(i)\big)}}{\ts}\,, \quad \forall p\,,\label{eq:speed}
\end{align}
with \(\bm{n}_p(i) \sim \mathcal{N}(0, \sigma^2_{\textnormal{obs}})\) an additive white noise.
Using the estimation techniques of Section~\ref{sec:joint}, one can get estimators of the two parameters of the Gamma distribution of the speed, \(\hat{k}\) and \(\hat{s}\), based on the observed speeds.

We can the that the zero-mean white noise has no effect on our estimators. Indeed, as shown in \eqref{eq:speed}, the white-noise from time \(i+1\) is "cancelled" with the white-noise from time \(i\). We hence subtract the bias (which follows the same distribution) of two biased measurements of the position, and it is then "cancelled".

\section{Sequential Monte Carlo method}
\label{sec:mcfilter}
We now would like to track the position of the fish and the predator, based on noisy measurements generated by \mintinline{matlab}{GenerateObservations}.
In this case, one must exlude a linear evolution, hence using a Kalman filter would give bad results; we can however use a sequential Monte Carlo  (SMC) method.
At each time step, we must generate a large number of samples from the posterior distribution, and use those samples to generate the next state,
as explained in \cite{filter}.
The SMC method does not require us to compute those samples at each time step.
The following notations will be used: \(\pdf(x_t \mid y_1, \ldots ,y_t)\) is the posterior density function at time $t$, and \(x_t^i\) is a sample from this posterior density.
We also define the sample set \(\sset_t = \{x_t^i\}\), for \(i = 1,2,\ldots,\np\).
Our goal is to use the observations in \(\sset_t\) and the noisy measurements of the position, \(y_{t+1}^p\) to compute the sample set \(\sset_{t+1}\).
After doing so, we first create the new sample set then correct its values using the noisy measurements.

\begin{step}[Initialization]
	When one first calls the function, the initial sample set \(\sset_1\) is empty.
	It is then filled using noisy observations \(y_1^p\), together with an additive zero-mean white-noise for each particle.
	This intial guess allows us to start from the only approximation of the position we know (that is, its observed value).
	To compute the initial orientation of the \(p\)th fish, we use again the noisy measurements from \(y_2^p\) and \(y_1^p\). Throughout the computation, we keep in memory all the sample sets, for each time step, and for each fish and predator. Since we work in a subset of \(\mathbb{R}^2\), each sample \(x_{t}^i\) is a 2-dimensional vector, that is: \(x_{t}^i = \begin{pmatrix}
	\xi & \upsilon
	\end{pmatrix}^{T}\).
\end{step}

\begin{step}[Prediction]
	To predict the samples at step \(t+1\), we can use the posterior density samples at step \(t\), using the prediction density \(\pdf(x_{t+1} \mid y_1,\ldots,y_t)\).
	We can compute the prediction set \(\tilde{\sset}_{t+1}\) of samples \(\tilde{x}_t^i \).
	One can show that these last samples are samples from \(\pdf(x_{t+1} \mid y_1,\ldots,y_t)\). \cite{anuj ref 5}
	
	In our case, we use the \mintinline{matlab}{StateUpdate} function to generate the samples at time \(t+1\), which gives the state model update of the position.
\end{step}

\begin{step}[Correction]
	We use the prediction set computed earlier, \(\tilde{\sset}_{t+1}\), to generate posterior samples from the posterior density function.
	The goal here is to compute weights \( \tilde{w}_{t+1}^i \) for each particle of this set.
	Those weights allow to give more importance to particles which have a higher probability to be close to the true value of the position at time \(t+1\).
	Hence, the particles which are further from the true value will have a lower weight.
	These weights are then normalized, which gives \(\tilde{w}_{t+1}^i\).
	One can show that if we resample from the set \(\tilde{\sset}_{t+1}\) with probabilities given by \( \{ \tilde{w}_{t+1}^1, \ldots, \tilde{w}_{t+1}^{\np} \} \), then the resulting values are samples from the posterior.
	Generating \(\np\) resamples, which we call \(x_{t+1}^i\), gives the sample set
	\begin{align}
	\sset_{t+1} &= \{ x_{t+1}^1, x_{t+1}^2, \ldots ,x_{t+1}^{\np} \}\,,
	\end{align}
	which is the corrected sample set at time \( t+1 \).
\end{step}

\begin{step}[Termination]
For each time step, we have computed the posterior sample set of the position for each fish and for the predator.
However, the true aim of the particle filter is to give a valid approximation of the noiseless value of the position of each fish.
One can get these approximations, for time \(t\) and fish \(p\), by computing the sample mean of the sample set \(\sset_{t}\), relative to the \(p\)th fish.
\end{step}

\section{Mean squared error analysis}
\label{sec:mseanalysis}
Using the particle filter of Section~\ref{sec:mcfilter}, one can perform several experiments in order to determine its robustness and dependance on various parameters of the model.
In order to quantify this, we use the mean square error, defined as
\begin{align}
E_{\textnormal{MSE}} = \frac{1}{NP} \sum_{p=1}^P \sum_{i=1}^N \norm{\bm{x}_p(i) - \hat{\bm{x}}_p(i)}_{2}^2\,.
\end{align}
Experiments were conducted for different values of three parameters.
The default values were \(w = 20\), \(P = 3\), \(N = 100\),  \(\np = 100\), \(\ts = 0.1\) and \(\sigma_{\textnormal{obs}} = 0.2\).
All experiments were repeated \(M = 15\) times.

First, the effect of varying the number of particles per fish was studied.
Simulations were run with \(\np = 1, 2, 5, 10, 20, 50, 100, 150, 200, 250\).
The results are shown on Figure~\ref{fig:Np}.
\begin{figure}[!htbp]
	\centering
	\scalebox{0.75}{\input{img/mse_np.tex}}
	\caption{Mean squared error resulting from the filtered position estimates, for varying values of \(\np\).}
	\label{fig:Np}
\end{figure}
The figure shows that the mean squared error barely depends on the value of \(\np\), if the parameter is sufficiently big (\(\np \ge 10\) seems to work).
We can also observe that the mean squared error resulting from simply passing the observed positions as estimates is lower than the error with the particle filter.

Next, we studied the influence of the sampling period \(\ts\).
Simulations were run with \(\ts = 0.01, 0.05, 0.1, 0.2, 0.5, 1, 2, 5\).
The results are shown on Figure~\ref{fig:t_s}.
\begin{figure}[!htbp]
	\centering
	\scalebox{0.75}{\input{img/mse_t_s.tex}}
	\caption{Mean squared error resulting from the filtered position estimates, for varying values of \(\ts\).}
	\label{fig:t_s}
\end{figure}
In line with physical intuition, the larger the sampling period, the less accurate the results become.
A sampling period of \(\ts = \SI{0.2}{\second}\) is the maximum acceptable value, with an average error around \(1\).
Again, we observe that simply passing the positional observations as estimated positions yields better results than using the conventional filter.

Finally, the last parameter whose influence was examined is the standard deviation on the observed values, \(\sigma_\textnormal{obs}\).
Simulations were run with \(\sigma_{\textnormal{obs}} = 0.001, 0.01, 0.1, 0.2, 0.5, 1, 2, 5\).
The results are shown on Figure~\ref{fig:sigma_obs}.
\begin{figure}[!htbp]
	\centering
	\scalebox{0.75}{\input{img/mse_sigma_obs.tex}}
	\caption{Mean squared error resulting from the filtered position estimates, for varying values of \(\sigma_\textnormal{obs}\).}
	\label{fig:sigma_obs}
\end{figure}

\section{Theoretical improvements for the filter}
\label{sec:improvements}

\section{Conclusion}
In Section~\ref{sec:s_est}, we looked at maximum likelihood estimation for the scale parameter of a Gamma distribution, \(s\).
We found the result to be \(\hat{s}_{\textnormal{ML}} = \frac{\widebar{V}}{k}\) in \eqref{eq:mlestimator}.
We proved some asymptotic properties of this estimator in Section~\ref{sec:prop}, showing that it is asymptotically unbiased, efficient, best asymptotically normal and consistent.
After establishing this, in Section~\ref{sec:joint}, we looked into estimating both parameters of the Gamma distribution jointly, using our previous estimator.
This led us to find that \(\hat{k}_{\textnormal{ML}}\) is the solution of \eqref{eq:mlk}.
This estimator has no closed form, hence in Section~\ref{sec:est_code} we turned to numerical methods to estimate it; comparing the maximum likelihood estimators with the method of moments estimators showed that both are unbiased (for both parameters), with the former generally having lower variance, while the latter can be useful as an initial guess for the maximum likelihood estimator.
Section~\ref{sec:fisher}, together with Appendix~\ref{app:fisher_information}, details an analytical derivation of the bound given by the Cramér--Rao inequality of Theorem~\ref{theo:CR}.
The simulation in Section~\ref{sec:ratio} showed empirical evidence of the efficiency of the maximum likelihood estimator.

In Section~\ref{sec:speed}, we established how to extract speed measurements from noisy observations of a fish's position, then applied the estimation techniques defined previously to parameterize this speed as a gamma-distributed random variable.
Section~\ref{sec:mcfilter} explains the theoretical base for particle filters, and links this to our practical implementation for this particular project.
Using this particular implementation, we were able to provide a simulation using both Matlab and Julia, whose results are summarized, interpreted and discussed in Section~\ref{sec:mseanalysis}.
Finally, in Section~\ref{sec:improvements}, we reflected on the shortcomings of the current implementation as pointed out in the previous section, and suggested an improved version of the SMC method.

\clearpage
\appendix

\section{Definitions of properties}
\label{app:defs}

\begin{defn}[Unbiased estimator]
	\label{def_unbiased}
	The Fisher estimator \(\hTheta = g(Z)\) of \(\theta\) is \emph{unbiased} if
	\begin{equation}
	m_{\hTheta; \theta} \coloneqq \expe{g(Z); \theta} = \theta\,, \quad \textnormal{for all } \theta\,.
	\end{equation}
\end{defn}

\begin{thm}[Cramér--Rao inequality]
	\label{theo:CR}
	If \(Z = (Z_1, \ldots, Z_N)^T\) with i.i.d. random variables \(Z_k\) and if its probability density function given by \(\pdf_Z(z; \theta) = \prod_{k=1}^{N} \pdf_{Z_k}(z_k; \theta)\) satisfies certain regularity conditions, then the covariance of any unbiased estimator \(\hTheta\) satisfies the \emph{Cramér--Rao inequality}
	\begin{equation}
	\cov \hTheta \succeq \fisher^{-1}(\theta)\,,
	\end{equation}
	where \(\fisher(\theta)\) is the \(p \times p\) \emph{Fisher information matrix},
	defined by
	\begin{equation}
	\big[\fisher(\theta)\big]_{i, j} \coloneqq -\expe{\frac{\partial^2 \ln \pdf_Z(z; \theta)}{\partial \theta_i \partial \theta_j}}\,.
	\label{information_matrix}
	\end{equation}
\end{thm}
\begin{defn}[Efficient estimator]
	\label{def_eff}
	An unbiased estimator is said to be \emph{efficient} if it reaches the Cramér--Rao bound for all values of \(\theta\), that is,
	\begin{equation}
	\cov \hTheta = \fisher^{-1}(\theta)\,, \quad \forall \theta\,.
	\end{equation}
\end{defn}

\begin{defn}[Best asymptotically normal estimator]
	\label{def_normal}
	A sequence of consistent estimators \(\{\hTheta_N(Z)\}_{N \in \mathbb{N}}\) of \(\theta\) is called \emph{best asymptotically normal} if
	\begin{equation}
	\sqrt{N} \left(\hTheta_N(Z) - \theta\right) \xrightarrow[N \to +\infty]{\mathcal{D}} \mathcal{N}(0, \Sigma)\,,
	\end{equation}
	for some minimal positive definite matrix \(\Sigma\).
\end{defn}

\begin{defn}[Consistent estimator]
	\label{def_consis}
	A sequence \(\{\hTheta_N(Z)\}_{N \in \mathbb{N}}\) of estimators of \(\theta\) is called \emph{consistent} if
	\begin{equation}
	\plim_{N \to +\infty} \hTheta_N(Z) = \theta\,.
	\end{equation}
	Theorem~9.1 of~\cite{wms} proves that this is equivalent to the statement that the MSE of the estimator converges to zero as \(N\) goes to infinity.
\end{defn}

\section{Omitted proofs}
The following is a proof of asymptotic normality for the estimator in \eqref{eq:mlestimator}, using the Lindeberg--Lévy formulation of the central limit theorem.
\begin{proof}
\label{app:banproof}
We want to prove that  \(\frac{1}{\sqrt{N}}\ell'(\theta_0) \sim \mathcal{N}\big(0, \fisher(\theta_0)\big)\).
We simplify notation by writing \(\pdf(v_i)\) instead of \(\pdf_{V_i}(v_i; k, \theta)\)
and using Euler's notation for partial derivatives~\cite{wiki:eulerdiff}.
First, we show that the expected value of \(\frac{1}{\sqrt{N}}\ell'(\theta_0)\) is zero.
\begin{align}
\expe{\frac{\ell'(\theta_0)}{\sqrt{N}}} &= \int_{-\infty}^{+\infty} \partial_{\theta_0} \left(\sum_{i=1}^N \frac{\ln \pdf(v_i)}{\sqrt{N}}\right) \pdf(v_i)\dif v_i\\
&= \frac{1}{\sqrt{N}} \sum_{i=1}^N \int_{-\infty}^{+\infty} \frac{\partial_\theta \pdf(v_i)}{\pdf(v_i)} \pdf(v_i)\dif v_i\\
&= \frac{1}{\sqrt{N}} \sum_{i=1}^N \int_{-\infty}^{+\infty} \frac{\partial_\theta \pdf(v_i)}{\pdf(v_i)} \pdf(v_i) \dif v_i\\
&= \frac{1}{\sqrt{N}} \sum_{i=1}^N \int_{-\infty}^{+\infty} \partial_\theta \pdf(v_i) \dif v_i\\
&= \frac{1}{\sqrt{N}} \sum_{i=1}^N \partial_\theta \int_{-\infty}^{+\infty} \pdf(v_i) \dif v_i\\
&= 0\,.
\end{align}

Next, we compute the variance of \(\frac{1}{N} \ell'(\theta_0)\) (for \(i = 1, \ldots, N\))
\begin{align}
\expe{\big(\partial_{\theta_0} \ln \pdf(v_i)\big)^2} &= \int_{-\infty}^{+\infty} \partial_{\theta_0} \ln \pdf(v_i) \frac{\partial_{\theta_0} \pdf(v_i)}{\pdf(v_i)} \pdf(v_i)\dif v_i\\
&= \int_{-\infty}^{+\infty} \partial_{\theta_0} \ln \pdf(v_i) \partial_{\theta_0} \pdf(v_i) \dif v_i\,.
\intertext{Using the product rule, we can then find}
&= \begin{multlined}[t]
- \int_{-\infty}^{+\infty} \partial_{\theta_0 \theta_0} \ln \pdf(v_i) \pdf(v_i) \dif v_i \\
+ \int_{-\infty}^{+\infty} \partial_{\theta_0} \big(\partial_{\theta_0} \ln \pdf(v_i) \pdf(v_i)\big) \dif v_i
\end{multlined}\\
&= - \expe{\partial_{\theta_0 \theta_0 } \ln \pdf(v_i)} + \partial_{\theta_0} \int_{-\infty}^{\infty} \frac{\partial_{\theta_0} \pdf(v_i)}{\pdf(v_i)} \pdf(v_i) \dif v_i\\
&= \fisher(\theta)\,,
\end{align}
where the last expression can be shown to be zero by a similar argument as the one used above for the expected value.
Knowing this, one easily finds
\begin{equation}
\var{\frac{\ell'(\theta_0)}{\sqrt{N}}} = \frac{1}{N} \var{\sum_{i=1}^N \partial_{\theta_0} \ln \pdf(v_i)} = \fisher(\theta_0)\,,
\end{equation}
since the random variables are i.i.d..
Using the Lindeberg--Lévy CLT, we thus have
\begin{equation}
\frac{\ell'(\theta_0)}{\sqrt{N}} \sim \mathcal{N}\big(0, \fisher(\theta_0)\big)\,.\qedhere
\end{equation}

\end{proof}

\section{Computation of the Fisher information matrix}
\label{app:fisher_information}
In order to derive an analytical expression for the Fisher information matrix, we start by computing various derivatives of the log-likelihood which will be needed later (writing \(\ell(v; k, s)\) instead of \(\ell(v_1, \ldots, v_N; k, s)\)):
\begin{align}
\frac{\partial \ell(v; k, s)}{\partial s} &= \frac{1}{s^2}\sum_{i=1}^{N} v_i - \frac{Nk}{s}\,,\\
\frac{\partial \ell(v; k, s)}{\partial k} &= \sum_{i=1}^{N} \ln v_i - N \ln s - N \dig(k)\,,\\
\frac{\partial^2\ell(v; k, s)}{\partial s^2} &= -\frac{2}{s^3}\sum_{i=1}^{N} v_i + \frac{Nk}{s^2}\,,\label{eq:00}\\
\frac{\partial^2 \ell(v; k, s)}{\partial k^2} &= -N\psi_1(k)\,,\label{eq:11}\\
\frac{\partial^2 \ell(v; k, s)}{\partial k\partial s} = \frac{\partial^2 \ell(v; k, s)}{\partial s\partial k}&= -\frac{N}{s}\,.
\end{align}
In order to find the entries of the Fisher matrix, one must take the expectation of the previous derivatives:
\begin{align}
\Big[\fisher(\theta)\Big]_{0, 0} &= -\expe{\frac{\partial^2 \ell(v; k, s)}{\partial s^2}} = - \expe{-\frac{2}{s^3}\sum_{i=1}^{N} v_i + \frac{Nk}{s^2}}\\
&= \frac{2}{s^3}N k s - \frac{Nk}{s^2} = \frac{Nk}{s^2}\,,\\
\Big[\fisher(\theta)\Big]_{0, 1} = \Big[\fisher(\theta)\Big]_{1, 0} &=  -\expe{\frac{\partial^2 \ell(v; k, s)}{\partial k \partial s}} = \frac{N}{s}\,,\\
\Big[\fisher(\theta)\Big]_{1, 1} &= -\expe{\frac{\partial^2 \ell(v; k, s)}{\partial k^2}} = -N \trig(k)\,.
\end{align}

\bibliography{report-linma1731-project}
\bibliographystyle{aomplain}

\end{document}
\endinput
